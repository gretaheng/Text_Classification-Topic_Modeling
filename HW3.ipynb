{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nytimesarticle import articleAPI\n",
    "api = articleAPI('tcrAZ92puwt0ApwLWpHkLjS9JIYb35SZ')\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import *\n",
    "import re \n",
    "import string\n",
    "\n",
    "# import NLTK package\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer, PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# import SpaCy package\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from spacy.tokenizer import Tokenizer\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = api.search( q = 'President Xi' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--1--\n",
      "\n",
      "status\n",
      "copyright\n",
      "response\n",
      "\n",
      "--2--\n",
      "\n",
      "docs\n",
      "meta\n",
      "\n",
      "--3--\n",
      "\n",
      "web_url\n",
      "snippet\n",
      "lead_paragraph\n",
      "blog\n",
      "source\n",
      "multimedia\n",
      "headline\n",
      "keywords\n",
      "pub_date\n",
      "document_type\n",
      "news_desk\n",
      "section_name\n",
      "byline\n",
      "type_of_material\n",
      "_id\n",
      "word_count\n",
      "score\n",
      "uri\n",
      "\n",
      "--4--\n",
      "\n",
      "https://www.nytimes.com/2018/12/18/briefing/president-xi-kpop-huawei.html\n",
      "Let us help you start your day.\n",
      "\n",
      "--5--\n",
      "\n",
      "https://www.nytimes.com/2018/12/18/briefing/president-xi-kpop-huawei.html\n",
      "https://www.nytimes.com/2018/02/26/world/asia/xi-jinping-career-highlights.html\n",
      "https://www.nytimes.com/2018/02/25/world/asia/xi-jinping-china.html\n",
      "https://www.nytimes.com/2019/01/13/opinion/kim-jong-un-trump-china.html\n",
      "https://www.nytimes.com/reuters/2019/01/23/technology/23reuters-china-markets-tech.html\n",
      "https://www.nytimes.com/2018/12/09/opinion/trade-war-truce-trump-china.html\n",
      "https://www.nytimes.com/2017/11/10/insider/the-last-time-president-xi-took-a-question-from-an-american-correspondent.html\n",
      "https://www.nytimes.com/aponline/2019/01/21/world/asia/ap-as-china-cambodia.html\n",
      "https://www.nytimes.com/reuters/2019/01/20/world/asia/20reuters-china-northkorea.html\n",
      "https://www.nytimes.com/2018/11/25/opinion/china-philippines-visit-apec-trump.html\n"
     ]
    }
   ],
   "source": [
    "mylink = []\n",
    "print('\\n--1--\\n')\n",
    "    \n",
    "for element in articles:\n",
    "    print(element)\n",
    "\n",
    "print('\\n--2--\\n')\n",
    "    \n",
    "for element in articles['response']:\n",
    "    print(element)\n",
    "\n",
    "print('\\n--3--\\n')\n",
    "    \n",
    "for element in articles['response']['docs'][0]:\n",
    "    print(element)\n",
    "\n",
    "print('\\n--4--\\n')\n",
    "\n",
    "print(articles['response']['docs'][0]['web_url'])\n",
    "print(articles['response']['docs'][0]['snippet'])\n",
    "\n",
    "print('\\n--5--\\n')\n",
    "\n",
    "for element in articles['response']['docs']:\n",
    "    mylink.append(element['web_url'])\n",
    "    print(element['web_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save articles about president Xi to a dictionary, with title as key and content as value\n",
    "mydic = {}\n",
    "for link in mylink:\n",
    "    xhtml = urlopen(link)\n",
    "    xsource = xhtml.read()\n",
    "    xsoup = BeautifulSoup(xsource, 'html.parser')\n",
    "    content_raw = xsoup.find_all(\"div\", {\"class\": \"css-u5vfum StoryBodyCompanionColumn\"})\n",
    "    mycontent = []\n",
    "    for p in content_raw:\n",
    "        mycontent.append(p.get_text())\n",
    "    mytitle = xsoup.head.title.get_text()\n",
    "    mydic[mytitle] = mycontent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for k,v in mydic.items():\n",
    "    for p in v:\n",
    "        text += p\n",
    "# Clean text\n",
    "# Reference https://stackoverflow.com/questions/14596884/remove-text-between-and-in-python\n",
    "text = re.sub(\"([\\[]).*?([\\]])\", \"\\g<1>\\g<2>\", text)\n",
    "text = text.replace(\"■\",\"\").replace(\"•\",\"\").replace(\"[\",\"\").replace(\"]\",\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a Use NLTK word_tokenize() to tokenize your text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK tokenize\n",
    "tokens_nltk = word_tokenize(text)\n",
    "text_nltk = nltk.Text(tokens_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1b Use SpaCy (or a language-specific tokenizer, if not in SpaCy) to tokenize your text. (You may want to refer to the file “Useful SpaCy Commands (Assignment 3).pdf” in Files.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpaCy tokenize\n",
    "tokens_spacy = tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c Provide the numbers of tokens for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The numebr of tokens using NLTK tokenizer is 1818 \n",
      " The numebr of tokens using SpaCy tokenizer is 4738\n"
     ]
    }
   ],
   "source": [
    "# Number of Tokens using NLTK\n",
    "num_tk_nltk = len(set(tokens_nltk))\n",
    "# Number of Tokens using SpaCy\n",
    "num_tk_spacy = len(set(tokens_spacy))\n",
    "print(\" The numebr of tokens using NLTK tokenizer is\", num_tk_nltk, \\\n",
    "      \"\\n The numebr of tokens using SpaCy tokenizer is\", num_tk_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d Provide a few examples of differences (if any). Using small subsets of the text can help you find examples of differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_nltk[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Want"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_spacy[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(', 'Want', 'to', 'get', 'this', 'briefing', 'by', 'email', '?', 'Here']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_nltk[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Want to get this briefing by email? Here’s the sign-up.)Good"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_spacy[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens_nltk[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens_spacy[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences:\n",
    "\n",
    "(1) SpaCy combines punctuations with words. Take the first word of tokens_spacy as an example. The SpaCy tokenizes it to '(Want' tokeni, but NLTK tokenizes to '(' and 'Want'. Because Spacy does not seperate punctuations from words, the number of tokens is larger than the NLTK one.\n",
    "\n",
    "(2) The format of results are also different. SpaCy tokenizes strings to spacy.tokens.span.Span. NLTK tokenizes strings to a list of strings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Use Regular Expressions (RegEx, python package “re”) to find: You may want to get the file “regular-expressions-cheat-sheet-v2.pdf” from Files to help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a All words beginning with a– and ending in –ing or –ed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'approved', 'answered', 'amazed', 'adding', 'assumed', 'aimed', 'accusing', 'allowing', 'agreed', 'anything', 'asiabriefing', 'announced', 'angered', 'accepting', 'asked', 'aging', 'argued', 'angling', 'added'}\n"
     ]
    }
   ],
   "source": [
    "# Words in nltk tokenized text\n",
    "nltk2a = set([x for x in tokens_nltk if re.search(\"^a.*(ing|ed)$\", x)])\n",
    "print(nltk2a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accusing', 'announced', 'angered', 'anything', 'adding', 'accepting', 'added', 'aging', 'angling', 'allowing', 'amazed', 'approved', 'aimed', 'agreed', 'asked', 'assumed', 'answered']\n"
     ]
    }
   ],
   "source": [
    "# Words in spacy tokenized text\n",
    "spacy2a = []\n",
    "for token in tokens_spacy:\n",
    "    word = token.text\n",
    "    if re.search(\"^a.*(ing|ed)$\", word) and word not in spacy2a:\n",
    "        spacy2a.append(word)\n",
    "print(spacy2a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'argued', 'asiabriefing'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Difference of results\n",
    "print(len(nltk2a))\n",
    "print(len(spacy2a))\n",
    "\n",
    "set(nltk2a) - set(spacy2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b All words with –ing– in the middle of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'increasingly', 'dealings', 'document.Washington', 'single', 'seemingly', 'Washington', 'making.China', 'contingency', 'LivingTips', 'Arlington', 'readings', 'mornings', 'rings', 'Jennings', 'things', 'singular', 'grudgingly', 'willingness'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words in nltk tokenized text\n",
    "nltk2b = set([x for x in tokens_nltk if re.search(\"^(?!ing).+ing.*(?<!ing)$\", x)])\n",
    "print(nltk2b)\n",
    "# Check results\n",
    "nltk2bcheck = set([x for x in tokens_nltk if (\"ing\" in x and x[:3] != 'ing' and x[-3:] != 'ing')])\n",
    "# Compare results\n",
    "nltk2bcheck == nltk2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['morning.', 'Jinping’s', 'willingness', 'growing,', 'contingency', 'LivingTips', 'rings?', 'living,”', 'mornings', 'asiabriefing@nytimes.com.HONG', 'Xiaoping,', 'single', 'crying.', 'meeting,', 'making,', 'Beijing’s', 'Arlington,', 'increasingly', 'document.Washington', 'anything,', 'Beijing,', 'singular', 'readings', 'Washington,', 'things', 'marching,', 'winning,', 'Washington', 'King’s', 'Jennings,', 'Jennings', 'Jinping.', 'seemingly', 'building,', 'grudgingly', 'dealings', 'flashing,', 'making.China', 'saying,”', 'Dayspring,', 'Beijing.', 'traveling,', 'expecting,', \"Beijing's\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words in spacy tokenized text\n",
    "spacy2b = []\n",
    "for token in tokens_spacy:\n",
    "    word = token.text\n",
    "    if re.search(\"^(?!ing).+ing.*(?<!ing)$\", word) and (word not in spacy2b):\n",
    "        spacy2b.append(word)\n",
    "print(spacy2b)\n",
    "# Check results\n",
    "spacy2bcheck = []\n",
    "for token in tokens_spacy:\n",
    "    word = token.text\n",
    "    if ('ing' in word) and (word[:3] != 'ing') and (word[-3:] != 'ing') and (word not in spacy2bcheck):\n",
    "        spacy2bcheck.append(word)\n",
    "# Compare results\n",
    "spacy2bcheck == spacy2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "44 \n",
      "\n",
      "Words in the results of SpaCy but not in NLTK's {'morning.', 'rings?', 'Jinping’s', 'living,”', \"Beijing's\", 'making,', 'expecting,', 'building,', 'Jennings,', 'King’s', 'asiabriefing@nytimes.com.HONG', 'Xiaoping,', 'Arlington,', 'Jinping.', 'Beijing.', 'Beijing’s', 'saying,”', 'traveling,', 'winning,', 'crying.', 'marching,', 'anything,', 'Beijing,', 'meeting,', 'Washington,', 'flashing,', 'growing,', 'Dayspring,'} \n",
      "\n",
      "Words in the results of NLTK but not in SpaCy's {'Arlington', 'rings'}\n"
     ]
    }
   ],
   "source": [
    "# Difference of results\n",
    "print(len(nltk2b))\n",
    "print(len(spacy2b), \"\\n\")\n",
    "\n",
    "print(\"Words in the results of SpaCy but not in NLTK's\", set(spacy2b) - set(nltk2b),\"\\n\")\n",
    "print(\"Words in the results of NLTK but not in SpaCy's\", set(nltk2b) - set(spacy2b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c c. After removing all punctuation, provide a count of all words in your text containing numbers or punctuation and letters (if any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(l):\n",
    "    results = []\n",
    "    for s in l:\n",
    "        s = re.sub(r'[^\\w\\s]','',s, re.UNICODE)\n",
    "        if s:\n",
    "            results.append(s)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d',\n",
       " 'f',\n",
       " 's',\n",
       " 'd',\n",
       " 'f',\n",
       " ' ',\n",
       " ' ',\n",
       " 'd',\n",
       " 's',\n",
       " 'f',\n",
       " 's',\n",
       " 'd',\n",
       " 'f',\n",
       " ' ',\n",
       " 'd',\n",
       " 's',\n",
       " 'f']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'dfsdf , dsfsdf, dsf'\n",
    "remove_punc(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation for NLTK text\n",
    "removedtext_nltk = remove_punc(tokens_nltk)\n",
    "\n",
    "# Remove punctuation for SpaCy text\n",
    "tokenlistSpacy = [token.text for token in tokens_spacy]\n",
    "removedtext_spacy = remove_punc(tokenlistSpacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_num_punc(l):\n",
    "    num = []\n",
    "    punc = []\n",
    "    for w in l:\n",
    "        n = re.search('\\d+', w)\n",
    "        p = re.search('[^\\w\\s]',w)\n",
    "        if n and w not in num:\n",
    "            num.append(w)\n",
    "        if p and p not in punc:\n",
    "            punc.append(w)\n",
    "    return num, punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nltk, punc_nltk = check_num_punc(removedtext_nltk)\n",
    "num_spacy, punc_spacy = check_num_punc(removedtext_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20 words containing numbers using NLTK text.\n",
      "There are 0 words containing punctuation using NLTK text.\n",
      "There are 21 words containing numbers using SpaCy text.\n",
      "There are 0 words containing punctuation using SpaCy text.\n"
     ]
    }
   ],
   "source": [
    "print('There are', len(num_nltk), 'words containing numbers using NLTK text.')\n",
    "print('There are', len(punc_nltk), 'words containing punctuation using NLTK text.')\n",
    "print('There are', len(num_spacy), 'words containing numbers using SpaCy text.')\n",
    "print('There are', len(punc_nltk), 'words containing punctuation using SpaCy text.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NLTK text, we get the following words containing numbers: ['40th', '2016', '10000', '240', '58', '2020', '52', '300', '1960s', '2012', '2013', '1', '14', '2024', '90day', '2014', '2009', '12', '2015', '13']\n",
      "Using SpaCy text, we get the following words containing numbers: ['40th', '2016', '10000', '240', '58', '2020', '52', '300', '1960s', '2012', '2013Controlling', '2013', '1', '14', '2024', '90day', '2014', '2009', '12', '2015', '13']\n"
     ]
    }
   ],
   "source": [
    "print('Using NLTK text, we get the following words containing numbers:', num_nltk)\n",
    "print('Using SpaCy text, we get the following words containing numbers:', num_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One word is in the results of SpaCy text but not in the results of NLTK text {'2013Controlling'}\n"
     ]
    }
   ],
   "source": [
    "print(\"One word is in the results of SpaCy text but not in the results of NLTK text\", set(num_spacy)-set(num_nltk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Provide a selection of different “stems” from the 3 NLTK stemmers: a. Porter b. Lancaster c. Snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_stemmer(l,stemmer):\n",
    "    result = []\n",
    "    for w in l:\n",
    "        s = stemmer.stem(w)\n",
    "        if s not in result and not s.isspace():\n",
    "            result.append(s)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porter\n",
    "stemmer1 = PorterStemmer()\n",
    "stemmer_results_1_nltk = save_stemmer(removedtext_nltk, stemmer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancaster\n",
    "stemmer2 = LancasterStemmer()\n",
    "stemmer_results_2_nltk = save_stemmer(removedtext_nltk, stemmer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowball\n",
    "stemmer3 = SnowballStemmer(\"english\")\n",
    "stemmer_results_3_nltk = save_stemmer(removedtext_nltk, stemmer3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words have different stems using various stemmers\n",
    "different_stemmer_origin_word = []\n",
    "for w in removedtext_nltk:\n",
    "    r1 = stemmer1.stem(w)\n",
    "    r2 = stemmer2.stem(w)\n",
    "    r3 = stemmer3.stem(w)\n",
    "    if not (r1 == r2 == r3):\n",
    "        different_stemmer_origin_word.append([w,r1,r2,r3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'thi', 'thi', 'this'],\n",
       " ['Here', 'here', 'her', 'here'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['defiance', 'defianc', 'defy', 'defianc'],\n",
       " ['Huawei', 'huawei', 'huawe', 'huawei'],\n",
       " ['corporate', 'corpor', 'corp', 'corpor'],\n",
       " ['culture', 'cultur', 'cult', 'cultur'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['In', 'In', 'in', 'in'],\n",
       " ['anniversary', 'anniversari', 'annivers', 'anniversari'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['liberalization', 'liber', 'lib', 'liber'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['delivered', 'deliv', 'del', 'deliv'],\n",
       " ['his', 'hi', 'his', 'his'],\n",
       " ['Communist', 'communist', 'commun', 'communist'],\n",
       " ['Party', 'parti', 'party', 'parti'],\n",
       " ['has', 'ha', 'has', 'has'],\n",
       " ['solely', 'sole', 'sol', 'sole'],\n",
       " ['country', 'countri', 'country', 'countri'],\n",
       " ['growth', 'growth', 'grow', 'growth'],\n",
       " ['policies', 'polici', 'policy', 'polici'],\n",
       " ['have', 'have', 'hav', 'have'],\n",
       " ['totally', 'total', 'tot', 'total'],\n",
       " ['He', 'He', 'he', 'he'],\n",
       " ['only', 'onli', 'on', 'onli'],\n",
       " ['obliquely', 'obliqu', 'obl', 'obliqu'],\n",
       " ['trade', 'trade', 'trad', 'trade'],\n",
       " ['US', 'US', 'us', 'us'],\n",
       " ['No', 'No', 'no', 'no'],\n",
       " ['one', 'one', 'on', 'one'],\n",
       " ['dictate', 'dictat', 'dict', 'dictat'],\n",
       " ['done', 'done', 'don', 'done'],\n",
       " ['hopes', 'hope', 'hop', 'hope'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['was', 'wa', 'was', 'was'],\n",
       " ['ready', 'readi', 'ready', 'readi'],\n",
       " ['announce', 'announc', 'annount', 'announc'],\n",
       " ['make', 'make', 'mak', 'make'],\n",
       " ['economy', 'economi', 'econom', 'economi'],\n",
       " ['more', 'more', 'mor', 'more'],\n",
       " ['marketfriendly', 'marketfriendli', 'marketfriend', 'marketfriend'],\n",
       " ['potential', 'potenti', 'pot', 'potenti'],\n",
       " ['compromises', 'compromis', 'comprom', 'compromis'],\n",
       " ['States', 'state', 'stat', 'state'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['defiance', 'defianc', 'defy', 'defianc'],\n",
       " ['drove', 'drove', 'drov', 'drove'],\n",
       " ['Asian', 'asian', 'as', 'asian'],\n",
       " ['Foundation', 'foundat', 'found', 'foundat'],\n",
       " ['legal', 'legal', 'leg', 'legal'],\n",
       " ['his', 'hi', 'his', 'his'],\n",
       " ['charity', 'chariti', 'char', 'chariti'],\n",
       " ['J', 'J', 'j', 'j'],\n",
       " ['Foundation', 'foundat', 'found', 'foundat'],\n",
       " ['will', 'will', 'wil', 'will'],\n",
       " ['close', 'close', 'clos', 'close'],\n",
       " ['family', 'famili', 'famy', 'famili'],\n",
       " ['using', 'use', 'us', 'use'],\n",
       " ['Tower', 'tower', 'tow', 'tower'],\n",
       " ['general', 'gener', 'gen', 'general'],\n",
       " ['Barbara', 'barbara', 'barbar', 'barbara'],\n",
       " ['announced', 'announc', 'annount', 'announc'],\n",
       " ['news', 'news', 'new', 'news'],\n",
       " ['filed', 'file', 'fil', 'file'],\n",
       " ['office', 'offic', 'off', 'offic'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['presidential', 'presidenti', 'presid', 'presidenti'],\n",
       " ['even', 'even', 'ev', 'even'],\n",
       " ['lawyer', 'lawyer', 'lawy', 'lawyer'],\n",
       " ['foundation', 'foundat', 'found', 'foundat'],\n",
       " ['closure', 'closur', 'clos', 'closur'],\n",
       " ['one', 'one', 'on', 'one'],\n",
       " ['organization', 'organ', 'org', 'organ'],\n",
       " ['general', 'gener', 'gen', 'general'],\n",
       " ['legal', 'legal', 'leg', 'legal'],\n",
       " ['A', 'A', 'a', 'a'],\n",
       " ['sentencing', 'sentenc', 'sent', 'sentenc'],\n",
       " ['Flynn', 'flynn', 'flyn', 'flynn'],\n",
       " ['national', 'nation', 'nat', 'nation'],\n",
       " ['security', 'secur', 'sec', 'secur'],\n",
       " ['adviser', 'advis', 'adv', 'advis'],\n",
       " ['harshly', 'harshli', 'harsh', 'harsh'],\n",
       " ['lying', 'lie', 'lying', 'lie'],\n",
       " ['federal', 'feder', 'fed', 'feder'],\n",
       " ['his', 'hi', 'his', 'his'],\n",
       " ['Russians', 'russian', 'russ', 'russian'],\n",
       " ['hiding', 'hide', 'hid', 'hide'],\n",
       " ['his', 'hi', 'his', 'his'],\n",
       " ['lobbying', 'lobbi', 'lobby', 'lobbi'],\n",
       " ['role', 'role', 'rol', 'role'],\n",
       " ['Russia', 'russia', 'russ', 'russia'],\n",
       " ['many', 'mani', 'many', 'mani'],\n",
       " ['US', 'US', 'us', 'us'],\n",
       " ['Senate', 'senat', 'sen', 'senat'],\n",
       " ['published', 'publish', 'publ', 'publish'],\n",
       " ['this', 'thi', 'thi', 'this'],\n",
       " ['Russia', 'russia', 'russ', 'russia'],\n",
       " ['interference', 'interfer', 'interf', 'interfer'],\n",
       " ['Social', 'social', 'soc', 'social'],\n",
       " ['media', 'media', 'med', 'media'],\n",
       " ['companies', 'compani', 'company', 'compani'],\n",
       " ['crucial', 'crucial', 'cruc', 'crucial'],\n",
       " ['cybersecurity', 'cybersecur', 'cybersec', 'cybersecur'],\n",
       " ['Senate', 'senat', 'sen', 'senat'],\n",
       " ['hired', 'hire', 'hir', 'hire'],\n",
       " ['trace', 'trace', 'trac', 'trace'],\n",
       " ['Russian', 'russian', 'russ', 'russian'],\n",
       " ['influence', 'influenc', 'influ', 'influenc'],\n",
       " ['companies', 'compani', 'company', 'compani'],\n",
       " ['extent', 'extent', 'ext', 'extent'],\n",
       " ['Russian', 'russian', 'russ', 'russian'],\n",
       " ['activity', 'activ', 'act', 'activ'],\n",
       " ['over', 'over', 'ov', 'over'],\n",
       " ['data', 'data', 'dat', 'data'],\n",
       " ['often', 'often', 'oft', 'often'],\n",
       " ['formats', 'format', 'form', 'format'],\n",
       " ['made', 'made', 'mad', 'made'],\n",
       " ['criticism', 'critic', 'crit', 'critic'],\n",
       " ['social', 'social', 'soc', 'social'],\n",
       " ['media', 'media', 'med', 'media'],\n",
       " ['companies', 'compani', 'company', 'compani'],\n",
       " ['willingness', 'willing', 'wil', 'willing'],\n",
       " ['role', 'role', 'rol', 'role'],\n",
       " ['Here', 'here', 'her', 'here'],\n",
       " ['exemplify', 'exemplifi', 'exempl', 'exemplifi'],\n",
       " ['Russia', 'russia', 'russ', 'russia'],\n",
       " ['nuisance', 'nuisanc', 'nuis', 'nuisanc'],\n",
       " ['fansFrenzied', 'fansfrenzi', 'fansfrenzy', 'fansfrenzi'],\n",
       " ['pictured', 'pictur', 'pict', 'pictur'],\n",
       " ['have', 'have', 'hav', 'have'],\n",
       " ['closer', 'closer', 'clos', 'closer'],\n",
       " ['planes', 'plane', 'plan', 'plane'],\n",
       " ['selfie', 'selfi', 'selfy', 'selfi'],\n",
       " ['before', 'befor', 'bef', 'befor'],\n",
       " ['gates', 'gate', 'gat', 'gate'],\n",
       " ['these', 'these', 'thes', 'these'],\n",
       " ['have', 'have', 'hav', 'have'],\n",
       " ['angered', 'anger', 'ang', 'anger'],\n",
       " ['officials', 'offici', 'off', 'offici'],\n",
       " ['departures', 'departur', 'depart', 'departur'],\n",
       " ['activate', 'activ', 'act', 'activ'],\n",
       " ['regulations', 'regul', 'reg', 'regul'],\n",
       " ['all', 'all', 'al', 'all'],\n",
       " ['have', 'have', 'hav', 'have'],\n",
       " ['plane', 'plane', 'plan', 'plane'],\n",
       " ['security', 'secur', 'sec', 'secur'],\n",
       " ['checkKorean', 'checkkorean', 'checkk', 'checkkorean'],\n",
       " ['announced', 'announc', 'annount', 'announc'],\n",
       " ['financial', 'financi', 'fin', 'financi'],\n",
       " ['penalties', 'penalti', 'penal', 'penalti'],\n",
       " ['unusual', 'unusu', 'unus', 'unusu'],\n",
       " ['North', 'north', 'nor', 'north'],\n",
       " ['Korea', 'korea', 'kore', 'korea'],\n",
       " ['A', 'A', 'a', 'a'],\n",
       " ['cited', 'cite', 'cit', 'cite'],\n",
       " ['circulation', 'circul', 'circ', 'circul'],\n",
       " ['wildly', 'wildli', 'wild', 'wild'],\n",
       " ['popular', 'popular', 'popul', 'popular'],\n",
       " ['music', 'music', 'mus', 'music'],\n",
       " ['one', 'one', 'on', 'one'],\n",
       " ['influences', 'influenc', 'influ', 'influenc'],\n",
       " ['have', 'have', 'hav', 'have'],\n",
       " ['country_____Business',\n",
       "  'country_____busi',\n",
       "  'country_____business',\n",
       "  'country_____busi'],\n",
       " ['Huawei', 'huawei', 'huawe', 'huawei'],\n",
       " ['employees', 'employe', 'employ', 'employe'],\n",
       " ['have', 'have', 'hav', 'have'],\n",
       " ['name', 'name', 'nam', 'name'],\n",
       " ['company', 'compani', 'company', 'compani'],\n",
       " ['corporate', 'corpor', 'corp', 'corpor'],\n",
       " ['culture', 'cultur', 'cult', 'cultur'],\n",
       " ['It', 'It', 'it', 'it'],\n",
       " ['company', 'compani', 'company', 'compani'],\n",
       " ['competitors', 'competitor', 'competit', 'competitor'],\n",
       " ['has', 'ha', 'has', 'has'],\n",
       " ['Selfdriving', 'selfdriv', 'selfdr', 'selfdriv'],\n",
       " ['carrying', 'carri', 'carry', 'carri'],\n",
       " ['are', 'are', 'ar', 'are'],\n",
       " ['reality', 'realiti', 'real', 'realiti'],\n",
       " ['some', 'some', 'som', 'some'],\n",
       " ['are', 'are', 'ar', 'are'],\n",
       " ['other', 'other', 'oth', 'other'],\n",
       " ['uses', 'use', 'us', 'use'],\n",
       " ['like', 'like', 'lik', 'like'],\n",
       " ['grocery', 'groceri', 'grocery', 'groceri'],\n",
       " ['deliveries', 'deliveri', 'delivery', 'deliveri'],\n",
       " ['journalist', 'journalist', 'journ', 'journalist'],\n",
       " ['has', 'ha', 'has', 'has'],\n",
       " ['My', 'My', 'my', 'my'],\n",
       " ['policies', 'polici', 'policy', 'polici'],\n",
       " ['actions', 'action', 'act', 'action'],\n",
       " ['have', 'have', 'hav', 'have'],\n",
       " ['where', 'where', 'wher', 'where'],\n",
       " ['I', 'I', 'i', 'i'],\n",
       " ['longer', 'longer', 'long', 'longer'],\n",
       " ['here', 'here', 'her', 'here'],\n",
       " ['wrote', 'wrote', 'wrot', 'wrote'],\n",
       " ['US', 'US', 'us', 'us'],\n",
       " ['were', 'were', 'wer', 'were'],\n",
       " ['Here', 'here', 'her', 'here'],\n",
       " ['global', 'global', 'glob', 'global'],\n",
       " ['News', 'new', 'new', 'news'],\n",
       " ['In', 'In', 'in', 'in'],\n",
       " ['region', 'region', 'reg', 'region'],\n",
       " ['India', 'india', 'ind', 'india'],\n",
       " ['Pakistan', 'pakistan', 'pak', 'pakistan'],\n",
       " ['were', 'were', 'wer', 'were'],\n",
       " ['killed', 'kill', 'kil', 'kill'],\n",
       " ['Indian', 'indian', 'ind', 'indian'],\n",
       " ['security', 'secur', 'sec', 'secur'],\n",
       " ['this', 'thi', 'thi', 'this'],\n",
       " ['more', 'more', 'mor', 'more'],\n",
       " ['civilian', 'civilian', 'civil', 'civilian'],\n",
       " ['toll', 'toll', 'tol', 'toll'],\n",
       " ['protestors', 'protestor', 'protest', 'protestor'],\n",
       " ['Indian', 'indian', 'ind', 'indian'],\n",
       " ['security', 'secur', 'sec', 'secur'],\n",
       " ['White', 'white', 'whit', 'white'],\n",
       " ['signaled', 'signal', 'sign', 'signal'],\n",
       " ['his', 'hi', 'his', 'his'],\n",
       " ['border', 'border', 'bord', 'border'],\n",
       " ['wall', 'wall', 'wal', 'wall'],\n",
       " ['British', 'british', 'brit', 'british'],\n",
       " ['Parliament', 'parliament', 'parlia', 'parliament'],\n",
       " ['contingency', 'conting', 'cont', 'conting'],\n",
       " ['chaotic', 'chaotic', 'chaot', 'chaotic'],\n",
       " ['departure', 'departur', 'depart', 'departur'],\n",
       " ['EU', 'EU', 'eu', 'eu'],\n",
       " ['standby', 'standbi', 'standby', 'standbi'],\n",
       " ['some', 'some', 'som', 'some'],\n",
       " ['speculated', 'specul', 'spec', 'specul'],\n",
       " ['unusually', 'unusu', 'unus', 'unusu'],\n",
       " ['public', 'public', 'publ', 'public'],\n",
       " ['announcement', 'announc', 'annount', 'announc'],\n",
       " ['was', 'wa', 'was', 'was'],\n",
       " ['scare', 'scare', 'scar', 'scare'],\n",
       " ['tactic', 'tactic', 'tact', 'tactic'],\n",
       " ['Parliament', 'parliament', 'parlia', 'parliament'],\n",
       " ['Prime', 'prime', 'prim', 'prime'],\n",
       " ['Minister', 'minist', 'min', 'minist'],\n",
       " ['Theresa', 'theresa', 'theres', 'theresa'],\n",
       " ['administration', 'administr', 'admin', 'administr'],\n",
       " ['enable', 'enabl', 'en', 'enabl'],\n",
       " ['semiautomatic', 'semiautomat', 'semiautom', 'semiautomat'],\n",
       " ['fire', 'fire', 'fir', 'fire'],\n",
       " ['gunman', 'gunman', 'gunm', 'gunman'],\n",
       " ['used', 'use', 'us', 'use'],\n",
       " ['Las', 'la', 'las', 'las'],\n",
       " ['Detainees', 'detaine', 'detain', 'detaine'],\n",
       " ['have', 'have', 'hav', 'have'],\n",
       " ['manufacturing', 'manufactur', 'manufact', 'manufactur'],\n",
       " ['US', 'US', 'us', 'us'],\n",
       " ['difficulty', 'difficulti', 'difficul', 'difficulti'],\n",
       " ['made', 'made', 'mad', 'made'],\n",
       " ['labor', 'labor', 'lab', 'labor'],\n",
       " ['entering', 'enter', 'ent', 'enter'],\n",
       " ['global', 'global', 'glob', 'global'],\n",
       " ['supply', 'suppli', 'supply', 'suppli'],\n",
       " ['will', 'will', 'wil', 'will'],\n",
       " ['referendum', 'referendum', 'referend', 'referendum'],\n",
       " ['legalizing', 'legal', 'leg', 'legal'],\n",
       " ['recreational', 'recreat', 'recr', 'recreat'],\n",
       " ['marijuana', 'marijuana', 'marijuan', 'marijuana'],\n",
       " ['officials', 'offici', 'off', 'offici'],\n",
       " ['apparently', 'appar', 'app', 'appar'],\n",
       " ['making', 'make', 'mak', 'make'],\n",
       " ['country', 'countri', 'country', 'countri'],\n",
       " ['vote', 'vote', 'vot', 'vote'],\n",
       " ['Drones', 'drone', 'dron', 'drone'],\n",
       " ['are', 'are', 'ar', 'are'],\n",
       " ['delivering', 'deliv', 'del', 'deliv'],\n",
       " ['children', 'children', 'childr', 'children'],\n",
       " ['South', 'south', 'sou', 'south'],\n",
       " ['Pacific', 'pacif', 'pac', 'pacif'],\n",
       " ['nation', 'nation', 'nat', 'nation'],\n",
       " ['Australia', 'australia', 'austral', 'australia'],\n",
       " ['being', 'be', 'being', 'be'],\n",
       " ['after', 'after', 'aft', 'after'],\n",
       " ['TV', 'TV', 'tv', 'tv'],\n",
       " ['documentary', 'documentari', 'docu', 'documentari'],\n",
       " ['was', 'wa', 'was', 'was'],\n",
       " ['using', 'use', 'us', 'use'],\n",
       " ['products', 'product', 'produc', 'product'],\n",
       " ['Smarter', 'smarter', 'smart', 'smarter'],\n",
       " ['more', 'more', 'mor', 'more'],\n",
       " ['life', 'life', 'lif', 'life'],\n",
       " ['Make', 'make', 'mak', 'make'],\n",
       " ['popular', 'popular', 'popul', 'popular'],\n",
       " ['chicken', 'chicken', 'chick', 'chicken'],\n",
       " ['ethical', 'ethic', 'eth', 'ethic'],\n",
       " ['Here', 'here', 'her', 'here'],\n",
       " ['Master', 'master', 'mast', 'master'],\n",
       " ['cruiseNoteworthy',\n",
       "  'cruisenoteworthi',\n",
       "  'cruisenoteworthy',\n",
       "  'cruisenoteworthi'],\n",
       " ['Places', 'place', 'plac', 'place'],\n",
       " ['energy', 'energi', 'energy', 'energi'],\n",
       " ['after', 'after', 'aft', 'after'],\n",
       " ['nearly', 'nearli', 'near', 'near'],\n",
       " ['beauty', 'beauti', 'beauty', 'beauti'],\n",
       " ['Bhutan', 'bhutan', 'bhut', 'bhutan'],\n",
       " ['where', 'where', 'wher', 'where'],\n",
       " ['cranes', 'crane', 'cran', 'crane'],\n",
       " ['heaven', 'heaven', 'heav', 'heaven'],\n",
       " ['Bhutan', 'bhutan', 'bhut', 'bhutan'],\n",
       " ['icy', 'ici', 'icy', 'ici'],\n",
       " ['million', 'million', 'mil', 'million'],\n",
       " ['sooner', 'sooner', 'soon', 'sooner'],\n",
       " ['scientists', 'scientist', 'sci', 'scientist'],\n",
       " ['Tinder', 'tinder', 'tind', 'tinder'],\n",
       " ['have', 'have', 'hav', 'have'],\n",
       " ['strategy', 'strategi', 'strategy', 'strategi'],\n",
       " ['Make', 'make', 'mak', 'make'],\n",
       " ['dating', 'date', 'dat', 'date'],\n",
       " ['StoryChina', 'storychina', 'storychin', 'storychina'],\n",
       " ['US', 'US', 'us', 'us'],\n",
       " ['have', 'have', 'hav', 'have'],\n",
       " ['declared', 'declar', 'decl', 'declar'],\n",
       " ['truce', 'truce', 'truc', 'truce'],\n",
       " ['trade', 'trade', 'trad', 'trade'],\n",
       " ['overThis', 'overthi', 'overth', 'overthi'],\n",
       " ['gives', 'give', 'giv', 'give'],\n",
       " ['language', 'languag', 'langu', 'languag'],\n",
       " ['English', 'english', 'engl', 'english'],\n",
       " ['trade', 'trade', 'trad', 'trade'],\n",
       " ['Germanic', 'german', 'germ', 'german'],\n",
       " ['usage', 'usag', 'us', 'usag'],\n",
       " ['living', 'live', 'liv', 'live'],\n",
       " ['carpentry', 'carpentri', 'carpentry', 'carpentri'],\n",
       " ['trade', 'trade', 'trad', 'trade'],\n",
       " ['Over', 'over', 'ov', 'over'],\n",
       " ['time', 'time', 'tim', 'time'],\n",
       " ['trade', 'trade', 'trad', 'trade'],\n",
       " ['selling', 'sell', 'sel', 'sell'],\n",
       " ['came', 'came', 'cam', 'came'],\n",
       " ['called', 'call', 'cal', 'call'],\n",
       " ['simply', 'simpli', 'simply', 'simpli'],\n",
       " ['trade', 'trade', 'trad', 'trade'],\n",
       " ['In', 'In', 'in', 'in'],\n",
       " ['trade', 'trade', 'trad', 'trade'],\n",
       " ['trade', 'trade', 'trad', 'trade'],\n",
       " ['pronounced', 'pronounc', 'pronount', 'pronounc'],\n",
       " ['trade', 'trade', 'trad', 'trade'],\n",
       " ['while', 'while', 'whil', 'while'],\n",
       " ['Yì', 'Yì', 'yì', 'yì'],\n",
       " ['has', 'ha', 'has', 'has'],\n",
       " ['easy', 'easi', 'easy', 'easi'],\n",
       " ['easy', 'easi', 'easy', 'easi'],\n",
       " ['Of', 'Of', 'of', 'of'],\n",
       " ['these', 'these', 'thes', 'these'],\n",
       " ['trade', 'trade', 'trad', 'trade'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['US', 'US', 'us', 'us'],\n",
       " ['butJennifer', 'butjennif', 'butjen', 'butjennif'],\n",
       " ['Jett', 'jett', 'jet', 'jett'],\n",
       " ['editor', 'editor', 'edit', 'editor'],\n",
       " ['office', 'offic', 'off', 'offic'],\n",
       " ['wrote', 'wrote', 'wrot', 'wrote'],\n",
       " ['published', 'publish', 'publ', 'publish'],\n",
       " ['updated', 'updat', 'upd', 'updat'],\n",
       " ['here', 'here', 'her', 'here'],\n",
       " ['Australian', 'australian', 'aust', 'australian'],\n",
       " ['Asian', 'asian', 'as', 'asian'],\n",
       " ['European', 'european', 'europ', 'european'],\n",
       " ['American', 'american', 'am', 'american'],\n",
       " ['Evening', 'even', 'ev', 'even'],\n",
       " ['US', 'US', 'us', 'us'],\n",
       " ['Australia', 'australia', 'austral', 'australia'],\n",
       " ['offers', 'offer', 'off', 'offer'],\n",
       " ['weekly', 'weekli', 'week', 'week'],\n",
       " ['letter', 'letter', 'let', 'letter'],\n",
       " ['analysis', 'analysi', 'analys', 'analysi'],\n",
       " ['readers', 'reader', 'read', 'reader'],\n",
       " ['full', 'full', 'ful', 'full'],\n",
       " ['Times', 'time', 'tim', 'time'],\n",
       " ['newsletters', 'newslett', 'newslet', 'newslett'],\n",
       " ['hereWhat', 'herewhat', 'herewh', 'herewhat'],\n",
       " ['like', 'like', 'lik', 'like'],\n",
       " ['here', 'here', 'her', 'here'],\n",
       " ['announcement', 'announc', 'annount', 'announc'],\n",
       " ['Communist', 'communist', 'commun', 'communist'],\n",
       " ['Party', 'parti', 'party', 'parti'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['moving', 'move', 'mov', 'move'],\n",
       " ['authority', 'author', 'auth', 'author'],\n",
       " ['his', 'hi', 'his', 'his'],\n",
       " ['itBy', 'itbi', 'itby', 'itbi'],\n",
       " ['leadership', 'leadership', 'lead', 'leadership'],\n",
       " ['was', 'wa', 'was', 'was'],\n",
       " ['place', 'place', 'plac', 'place'],\n",
       " ['after', 'after', 'aft', 'after'],\n",
       " ['oneman', 'oneman', 'onem', 'oneman'],\n",
       " ['rule', 'rule', 'rul', 'rule'],\n",
       " ['power', 'power', 'pow', 'power'],\n",
       " ['struggles', 'struggl', 'struggles', 'struggl'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['has', 'ha', 'has', 'has'],\n",
       " ['rule', 'rule', 'rul', 'rule'],\n",
       " ['older', 'older', 'old', 'older'],\n",
       " ['leader', 'leader', 'lead', 'leader'],\n",
       " ['ruling', 'rule', 'rul', 'rule'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['will', 'will', 'wil', 'will'],\n",
       " ['rule', 'rule', 'rul', 'rule'],\n",
       " ['come', 'come', 'com', 'come'],\n",
       " ['has', 'ha', 'has', 'has'],\n",
       " ['home', 'home', 'hom', 'home'],\n",
       " ['potential', 'potenti', 'pot', 'potenti'],\n",
       " ['truculence', 'trucul', 'truc', 'trucul'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['Communist', 'communist', 'commun', 'communist'],\n",
       " ['Party', 'parti', 'party', 'parti'],\n",
       " ['came', 'came', 'cam', 'came'],\n",
       " ['power', 'power', 'pow', 'power'],\n",
       " ['history', 'histori', 'hist', 'histori'],\n",
       " ['his', 'hi', 'his', 'his'],\n",
       " ['deftly', 'deftli', 'deft', 'deft'],\n",
       " ['rivals', 'rival', 'riv', 'rival'],\n",
       " ['rolling', 'roll', 'rol', 'roll'],\n",
       " ['over', 'over', 'ov', 'over'],\n",
       " ['dissent', 'dissent', 'diss', 'dissent'],\n",
       " ['cultivated', 'cultiv', 'cult', 'cultiv'],\n",
       " ['alliances', 'allianc', 'al', 'allianc'],\n",
       " ['strengthened', 'strengthen', 'strengthened', 'strengthen'],\n",
       " ['his', 'hi', 'his', 'his'],\n",
       " ['over', 'over', 'ov', 'over'],\n",
       " ['economy', 'economi', 'econom', 'economi'],\n",
       " ['militaryHere', 'militaryher', 'militaryh', 'militaryher'],\n",
       " ['Times', 'time', 'tim', 'time'],\n",
       " ['coverage', 'coverag', 'cov', 'coverag'],\n",
       " ['moments', 'moment', 'mom', 'moment'],\n",
       " ['his', 'hi', 'his', 'his'],\n",
       " ['life', 'life', 'lif', 'life'],\n",
       " ['Communist', 'communist', 'commun', 'communist'],\n",
       " ['Party', 'parti', 'party', 'parti'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['revolutionary', 'revolutionari', 'revolv', 'revolutionari'],\n",
       " ['leader', 'leader', 'lead', 'leader'],\n",
       " ['under', 'under', 'und', 'under'],\n",
       " ['vice', 'vice', 'vic', 'vice'],\n",
       " ['premier', 'premier', 'premy', 'premier'],\n",
       " ['elder', 'elder', 'eld', 'elder'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['was', 'wa', 'was', 'was'],\n",
       " ['1960s', '1960', '1960s', '1960s'],\n",
       " ['his', 'hi', 'his', 'his'],\n",
       " ['family', 'famili', 'famy', 'famili'],\n",
       " ['suffered', 'suffer', 'suff', 'suffer'],\n",
       " ['A', 'A', 'a', 'a'],\n",
       " ['bookish', 'bookish', 'book', 'bookish'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['hardened', 'harden', 'hard', 'harden'],\n",
       " ['combative', 'comb', 'comb', 'combat'],\n",
       " ['survivor', 'survivor', 'surv', 'survivor'],\n",
       " ['during', 'dure', 'dur', 'dure'],\n",
       " ['Cultural', 'cultur', 'cult', 'cultur'],\n",
       " ['Revolution', 'revolut', 'revolv', 'revolut'],\n",
       " ['was', 'wa', 'was', 'was'],\n",
       " ['In', 'In', 'in', 'in'],\n",
       " ['his', 'hi', 'his', 'his'],\n",
       " ['experiences', 'experi', 'expery', 'experi'],\n",
       " ['during', 'dure', 'dur', 'dure'],\n",
       " ['Cultural', 'cultur', 'cult', 'cultur'],\n",
       " ['Revolution', 'revolut', 'revolv', 'revolut'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['was', 'wa', 'was', 'was'],\n",
       " ['smiling', 'smile', 'smil', 'smile'],\n",
       " ['was', 'wa', 'was', 'was'],\n",
       " ['millions', 'million', 'mil', 'million'],\n",
       " ['other', 'other', 'oth', 'other'],\n",
       " ['youth', 'youth', 'you', 'youth'],\n",
       " ['while', 'while', 'whil', 'while'],\n",
       " ['Beijing', 'beij', 'beid', 'beij'],\n",
       " ['was', 'wa', 'was', 'was'],\n",
       " ['crying', 'cri', 'cry', 'cri'],\n",
       " ['He', 'He', 'he', 'he'],\n",
       " ['was', 'wa', 'was', 'was'],\n",
       " ['happy', 'happi', 'happy', 'happi'],\n",
       " ['early', 'earli', 'ear', 'earli'],\n",
       " ['After', 'after', 'aft', 'after'],\n",
       " ['Cultural', 'cultur', 'cult', 'cultur'],\n",
       " ['Revolution', 'revolut', 'revolv', 'revolut'],\n",
       " ['were', 'were', 'wer', 'were'],\n",
       " ['still', 'still', 'stil', 'still'],\n",
       " ['trying', 'tri', 'try', 'tri'],\n",
       " ['cities', 'citi', 'city', 'citi'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['made', 'made', 'mad', 'made'],\n",
       " ['unusual', 'unusu', 'unus', 'unusu'],\n",
       " ['given', 'given', 'giv', 'given'],\n",
       " ['leadership', 'leadership', 'lead', 'leadership'],\n",
       " ['rural', 'rural', 'rur', 'rural'],\n",
       " ['area', 'area', 'are', 'area'],\n",
       " ['move', 'move', 'mov', 'move'],\n",
       " ['revealed', 'reveal', 'rev', 'reveal'],\n",
       " ['his', 'hi', 'his', 'his'],\n",
       " ['shrewdness', 'shrewd', 'shrewdness', 'shrewd'],\n",
       " ['Communist', 'communist', 'commun', 'communist'],\n",
       " ['Party', 'parti', 'party', 'parti'],\n",
       " ['local', 'local', 'loc', 'local'],\n",
       " ['leadership', 'leadership', 'lead', 'leadership'],\n",
       " ['Later', 'later', 'lat', 'later'],\n",
       " ['Fujian', 'fujian', 'fuj', 'fujian'],\n",
       " ['coastal', 'coastal', 'coast', 'coastal'],\n",
       " ['provinces', 'provinc', 'provint', 'provinc'],\n",
       " ['have', 'have', 'hav', 'have'],\n",
       " ['centers', 'center', 'cent', 'center'],\n",
       " ['growth', 'growth', 'grow', 'growth'],\n",
       " ['There', 'there', 'ther', 'there'],\n",
       " ['pragmatic', 'pragmat', 'pragm', 'pragmat'],\n",
       " ['style', 'style', 'styl', 'style'],\n",
       " ['ability', 'abil', 'abl', 'abil'],\n",
       " ['balance', 'balanc', 'bal', 'balanc'],\n",
       " ['powerful', 'power', 'pow', 'power'],\n",
       " ['officials', 'offici', 'off', 'offici'],\n",
       " ['centralizing', 'central', 'cent', 'central'],\n",
       " ['power', 'power', 'pow', 'power'],\n",
       " ['After', 'after', 'aft', 'after'],\n",
       " ['rising', 'rise', 'ris', 'rise'],\n",
       " ['Communist', 'communist', 'commun', 'communist'],\n",
       " ['Party', 'parti', 'party', 'parti'],\n",
       " ['later', 'later', 'lat', 'later'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['began', 'began', 'beg', 'began'],\n",
       " ['many', 'mani', 'many', 'mani'],\n",
       " ['rivals', 'rival', 'riv', 'rival'],\n",
       " ['tigers', 'tiger', 'tig', 'tiger'],\n",
       " ['punished', 'punish', 'pun', 'punish'],\n",
       " ['lower', 'lower', 'low', 'lower'],\n",
       " ['officials', 'offici', 'off', 'offici'],\n",
       " ['dangerous', 'danger', 'dang', 'danger'],\n",
       " ['tiger', 'tiger', 'tig', 'tiger'],\n",
       " ['former', 'former', 'form', 'former'],\n",
       " ['security', 'secur', 'sec', 'secur'],\n",
       " ['was', 'wa', 'was', 'was'],\n",
       " ['sentenced', 'sentenc', 'sent', 'sentenc'],\n",
       " ['life', 'life', 'lif', 'life'],\n",
       " ['power', 'power', 'pow', 'power'],\n",
       " ['accepting', 'accept', 'acceiv', 'accept'],\n",
       " ['bribes', 'bribe', 'brib', 'bribe'],\n",
       " ['revealing', 'reveal', 'rev', 'reveal'],\n",
       " ['state', 'state', 'stat', 'state'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['was', 'wa', 'was', 'was'],\n",
       " ['active', 'activ', 'act', 'activ'],\n",
       " ['member', 'member', 'memb', 'member'],\n",
       " ['Committee', 'committe', 'commit', 'committe'],\n",
       " ['face', 'face', 'fac', 'face'],\n",
       " ['While', 'while', 'whil', 'while'],\n",
       " ['announcement', 'announc', 'annount', 'announc'],\n",
       " ['was', 'wa', 'was', 'was'],\n",
       " ['surprise', 'surpris', 'surpr', 'surpris'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['has', 'ha', 'has', 'has'],\n",
       " ['signals', 'signal', 'sign', 'signal'],\n",
       " ['longer', 'longer', 'long', 'longer'],\n",
       " ['his', 'hi', 'his', 'his'],\n",
       " ['predecessors', 'predecessor', 'predecess', 'predecessor'],\n",
       " ['Hu', 'Hu', 'hu', 'hu'],\n",
       " ['fall', 'fall', 'fal', 'fall'],\n",
       " ['Communist', 'communist', 'commun', 'communist'],\n",
       " ['Party', 'parti', 'party', 'parti'],\n",
       " ['revealed', 'reveal', 'rev', 'reveal'],\n",
       " ['Committee', 'committe', 'commit', 'committe'],\n",
       " ['one', 'one', 'on', 'one'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['was', 'wa', 'was', 'was'],\n",
       " ['his', 'hi', 'his', 'his'],\n",
       " ['apparent', 'appar', 'app', 'appar'],\n",
       " ['At', 'At', 'at', 'at'],\n",
       " ['same', 'same', 'sam', 'same'],\n",
       " ['party', 'parti', 'party', 'parti'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['status', 'statu', 'stat', 'status'],\n",
       " ['equivalent', 'equival', 'equ', 'equival'],\n",
       " ['softer', 'softer', 'soft', 'softer'],\n",
       " ['image', 'imag', 'im', 'imag'],\n",
       " ['While', 'while', 'whil', 'while'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['has', 'ha', 'has', 'has'],\n",
       " ['used', 'use', 'us', 'use'],\n",
       " ['steely', 'steeli', 'ste', 'steeli'],\n",
       " ['power', 'power', 'pow', 'power'],\n",
       " ['has', 'ha', 'has', 'has'],\n",
       " ['image', 'imag', 'im', 'imag'],\n",
       " ['softer', 'softer', 'soft', 'softer'],\n",
       " ['more', 'more', 'mor', 'more'],\n",
       " ['human', 'human', 'hum', 'human'],\n",
       " ['leader', 'leader', 'lead', 'leader'],\n",
       " ['his', 'hi', 'his', 'his'],\n",
       " ['predecessors', 'predecessor', 'predecess', 'predecessor'],\n",
       " ['He', 'He', 'he', 'he'],\n",
       " ['has', 'ha', 'has', 'has'],\n",
       " ['made', 'made', 'mad', 'made'],\n",
       " ['casual', 'casual', 'cas', 'casual'],\n",
       " ['were', 'were', 'wer', 'were'],\n",
       " ['rare', 'rare', 'rar', 'rare'],\n",
       " ['leaders', 'leader', 'lead', 'leader'],\n",
       " ['military', 'militari', 'milit', 'militari'],\n",
       " ['leadership', 'leadership', 'lead', 'leadership'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['has', 'ha', 'has', 'has'],\n",
       " ['moved', 'move', 'mov', 'move'],\n",
       " ['over', 'over', 'ov', 'over'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['military', 'militari', 'milit', 'militari'],\n",
       " ['generals', 'gener', 'gen', 'general'],\n",
       " ['shrinking', 'shrink', 'shrinking', 'shrink'],\n",
       " ['numbers', 'number', 'numb', 'number'],\n",
       " ['while', 'while', 'whil', 'while'],\n",
       " ['more', 'more', 'mor', 'more'],\n",
       " ['ability', 'abil', 'abl', 'abil'],\n",
       " ['naval', 'naval', 'nav', 'naval'],\n",
       " ['power', 'power', 'pow', 'power'],\n",
       " ['He', 'He', 'he', 'he'],\n",
       " ['has', 'ha', 'has', 'has'],\n",
       " ['central', 'central', 'cent', 'central'],\n",
       " ['role', 'role', 'rol', 'role'],\n",
       " ['decision', 'decis', 'decid', 'decis'],\n",
       " ['making', 'make', 'mak', 'make'],\n",
       " ['role', 'role', 'rol', 'role'],\n",
       " ['has', 'ha', 'has', 'has'],\n",
       " ['taken', 'taken', 'tak', 'taken'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['premier', 'premier', 'premy', 'premier'],\n",
       " ['His', 'hi', 'his', 'his'],\n",
       " ['infrastructure', 'infrastructur', 'infrastruct', 'infrastructur'],\n",
       " ['under', 'under', 'und', 'under'],\n",
       " ['initiative', 'initi', 'in', 'initi'],\n",
       " ['use', 'use', 'us', 'use'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['wealth', 'wealth', 'weal', 'wealth'],\n",
       " ['industrial', 'industri', 'indust', 'industri'],\n",
       " ['create', 'creat', 'cre', 'creat'],\n",
       " ['globalization', 'global', 'glob', 'global'],\n",
       " ['will', 'will', 'wil', 'will'],\n",
       " ['rules', 'rule', 'rul', 'rule'],\n",
       " ['aging', 'age', 'ag', 'age'],\n",
       " ['dissent', 'dissent', 'diss', 'dissent'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['has', 'ha', 'has', 'has'],\n",
       " ['carried', 'carri', 'carry', 'carri'],\n",
       " ['dissent', 'dissent', 'diss', 'dissent'],\n",
       " ['lawyers', 'lawyer', 'lawy', 'lawyer'],\n",
       " ['journalists', 'journalist', 'journ', 'journalist'],\n",
       " ['activists', 'activist', 'act', 'activist'],\n",
       " ['party', 'parti', 'party', 'parti'],\n",
       " ['Prize', 'prize', 'priz', 'prize'],\n",
       " ['laureate', 'laureat', 'laur', 'laureat'],\n",
       " ['was', 'wa', 'was', 'was'],\n",
       " ['organizing', 'organ', 'org', 'organ'],\n",
       " ['prodemocracy', 'prodemocraci', 'prodemocr', 'prodemocraci'],\n",
       " ['died', 'die', 'died', 'die'],\n",
       " ['under', 'under', 'und', 'under'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['has', 'ha', 'has', 'has'],\n",
       " ['taken', 'taken', 'tak', 'taken'],\n",
       " ['stance', 'stanc', 'stant', 'stanc'],\n",
       " ['Taiwan', 'taiwan', 'taiw', 'taiwan'],\n",
       " ['selfruled', 'selfrul', 'selfr', 'selfrul'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['territory', 'territori', 'territ', 'territori'],\n",
       " ['city', 'citi', 'city', 'citi'],\n",
       " ['where', 'where', 'wher', 'where'],\n",
       " ['have', 'have', 'hav', 'have'],\n",
       " ['Beijing', 'beij', 'beid', 'beij'],\n",
       " ['influence', 'influenc', 'influ', 'influenc'],\n",
       " ['In', 'In', 'in', 'in'],\n",
       " ['any', 'ani', 'any', 'ani'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['sovereignty', 'sovereignti', 'sovereignty', 'sovereignti'],\n",
       " ['security', 'secur', 'sec', 'secur'],\n",
       " ['power', 'power', 'pow', 'power'],\n",
       " ['central', 'central', 'cent', 'central'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['his', 'hi', 'his', 'his'],\n",
       " ['rule', 'rule', 'rul', 'rule'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['leader', 'leader', 'lead', 'leader'],\n",
       " ['announced', 'announc', 'annount', 'announc'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['era', 'era', 'er', 'era'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['has', 'ha', 'has', 'has'],\n",
       " ['since', 'sinc', 'sint', 'sinc'],\n",
       " ['has', 'ha', 'has', 'has'],\n",
       " ['cultivate', 'cultiv', 'cult', 'cultiv'],\n",
       " ['image', 'imag', 'im', 'imag'],\n",
       " ['father', 'father', 'fath', 'father'],\n",
       " ['figure', 'figur', 'fig', 'figur'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['ruling', 'rule', 'rul', 'rule'],\n",
       " ['Communist', 'communist', 'commun', 'communist'],\n",
       " ['Party', 'parti', 'party', 'parti'],\n",
       " ['decision', 'decis', 'decid', 'decis'],\n",
       " ['open', 'open', 'op', 'open'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['heightened', 'heighten', 'height', 'heighten'],\n",
       " ['resentment', 'resent', 'res', 'resent'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['lawyers', 'lawyer', 'lawy', 'lawyer'],\n",
       " ['journalists', 'journalist', 'journ', 'journalist'],\n",
       " ['business', 'busi', 'busy', 'busi'],\n",
       " ['Many', 'mani', 'many', 'mani'],\n",
       " ['have', 'have', 'hav', 'have'],\n",
       " ['warily', 'warili', 'wary', 'warili'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['has', 'ha', 'has', 'has'],\n",
       " ['used', 'use', 'us', 'use'],\n",
       " ['his', 'hi', 'his', 'his'],\n",
       " ['power', 'power', 'pow', 'power'],\n",
       " ['scores', 'score', 'scor', 'score'],\n",
       " ['free', 'free', 'fre', 'free'],\n",
       " ['tighten', 'tighten', 'tight', 'tighten'],\n",
       " ['economy', 'economi', 'econom', 'economi'],\n",
       " ['Beijing', 'beij', 'beid', 'beij'],\n",
       " ['critical', 'critic', 'crit', 'critic'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['He', 'He', 'he', 'he'],\n",
       " ['will', 'will', 'wil', 'will'],\n",
       " ['have', 'have', 'hav', 'have'],\n",
       " ['his', 'hi', 'his', 'his'],\n",
       " ['power', 'power', 'pow', 'power'],\n",
       " ['censors', 'censor', 'cens', 'censor'],\n",
       " ['criticism', 'critic', 'crit', 'critic'],\n",
       " ['decision', 'decis', 'decid', 'decis'],\n",
       " ['memes', 'meme', 'mem', 'meme'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['emperor', 'emperor', 'emp', 'emperor'],\n",
       " ['rule', 'rule', 'rul', 'rule'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['image', 'imag', 'im', 'imag'],\n",
       " ['Tiananmen', 'tiananmen', 'tiananm', 'tiananmen'],\n",
       " ['Square', 'squar', 'squ', 'squar'],\n",
       " ['line', 'line', 'lin', 'line'],\n",
       " ['Twice', 'twice', 'twic', 'twice'],\n",
       " ['poke', 'poke', 'pok', 'poke'],\n",
       " ['idea', 'idea', 'ide', 'idea'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['party', 'parti', 'party', 'parti'],\n",
       " ['move', 'move', 'mov', 'move'],\n",
       " ['comes', 'come', 'com', 'come'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['has', 'ha', 'has', 'has'],\n",
       " ['era', 'era', 'er', 'era'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['greatness', 'great', 'gre', 'great'],\n",
       " ['country', 'countri', 'country', 'countri'],\n",
       " ['will', 'will', 'wil', 'will'],\n",
       " ['take', 'take', 'tak', 'take'],\n",
       " ['place', 'place', 'plac', 'place'],\n",
       " ['global', 'global', 'glob', 'global'],\n",
       " ['power', 'power', 'pow', 'power'],\n",
       " ['Already', 'alreadi', 'already', 'alreadi'],\n",
       " ['establishing', 'establish', 'est', 'establish'],\n",
       " ['military', 'militari', 'milit', 'militari'],\n",
       " ['bases', 'base', 'bas', 'base'],\n",
       " ['Pacific', 'pacif', 'pac', 'pacif'],\n",
       " ['Africa', 'africa', 'afric', 'africa'],\n",
       " ['infrastructure', 'infrastructur', 'infrastruct', 'infrastructur'],\n",
       " ['Asia', 'asia', 'as', 'asia'],\n",
       " ['Africa', 'africa', 'afric', 'africa'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['hopes', 'hope', 'hop', 'hope'],\n",
       " ['will', 'will', 'wil', 'will'],\n",
       " ['No', 'No', 'no', 'no'],\n",
       " ['economy', 'economi', 'econom', 'economi'],\n",
       " ['sooner', 'sooner', 'soon', 'sooner'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['great', 'great', 'gre', 'great'],\n",
       " ['power', 'power', 'pow', 'power'],\n",
       " ['status', 'statu', 'stat', 'status'],\n",
       " ['trajectory', 'trajectori', 'traject', 'trajectori'],\n",
       " ['are', 'are', 'ar', 'are'],\n",
       " ['director', 'director', 'direct', 'director'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['Studies', 'studi', 'study', 'studi'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['worried', 'worri', 'worry', 'worri'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['oneman', 'oneman', 'onem', 'oneman'],\n",
       " ['rule', 'rule', 'rul', 'rule'],\n",
       " ['worsen', 'worsen', 'wors', 'worsen'],\n",
       " ['increasingly', 'increasingli', 'increas', 'increas'],\n",
       " ['relationship', 'relationship', 'rel', 'relationship'],\n",
       " ['States', 'state', 'stat', 'state'],\n",
       " ['States', 'state', 'stat', 'state'],\n",
       " ['engage', 'engag', 'eng', 'engag'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['climate', 'climat', 'clim', 'climat'],\n",
       " ['human', 'human', 'hum', 'human'],\n",
       " ['administration', 'administr', 'admin', 'administr'],\n",
       " ['Beijing', 'beij', 'beid', 'beij'],\n",
       " ['called', 'call', 'cal', 'call'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['strategic', 'strateg', 'strategic', 'strateg'],\n",
       " ['competitor', 'competitor', 'competit', 'competitor'],\n",
       " ['national', 'nation', 'nat', 'nation'],\n",
       " ['security', 'secur', 'sec', 'secur'],\n",
       " ['policy', 'polici', 'policy', 'polici'],\n",
       " ['makers', 'maker', 'mak', 'maker'],\n",
       " ['are', 'are', 'ar', 'are'],\n",
       " ['preparing', 'prepar', 'prep', 'prepar'],\n",
       " ['some', 'some', 'som', 'some'],\n",
       " ['States', 'state', 'stat', 'state'],\n",
       " ['particularly', 'particularli', 'particul', 'particular'],\n",
       " ['more', 'more', 'mor', 'more'],\n",
       " ['States', 'state', 'stat', 'state'],\n",
       " ['military', 'militari', 'milit', 'militari'],\n",
       " ['advantage', 'advantag', 'adv', 'advantag'],\n",
       " ['over', 'over', 'ov', 'over'],\n",
       " ['Liberation', 'liber', 'lib', 'liber'],\n",
       " ['congressional', 'congression', 'congress', 'congression'],\n",
       " ['testimony', 'testimoni', 'testimony', 'testimoni'],\n",
       " ['earlier', 'earlier', 'ear', 'earlier'],\n",
       " ['this', 'thi', 'thi', 'this'],\n",
       " ['month', 'month', 'mon', 'month'],\n",
       " ['director', 'director', 'direct', 'director'],\n",
       " ['Christopher', 'christoph', 'christopher', 'christoph'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['whole', 'whole', 'whol', 'whole'],\n",
       " ['whole', 'whole', 'whol', 'whole'],\n",
       " ['society', 'societi', 'socy', 'societi'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['well', 'well', 'wel', 'well'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['power', 'power', 'pow', 'power'],\n",
       " ['global', 'global', 'glob', 'global'],\n",
       " ['increasingly', 'increasingli', 'increas', 'increas'],\n",
       " ['influential', 'influenti', 'influ', 'influenti'],\n",
       " ['leaders', 'leader', 'lead', 'leader'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['V', 'V', 'v', 'v'],\n",
       " ['Russian', 'russian', 'russ', 'russian'],\n",
       " ['leader', 'leader', 'lead', 'leader'],\n",
       " ['James', 'jame', 'jam', 'jame'],\n",
       " ['Mann', 'mann', 'man', 'mann'],\n",
       " ['author', 'author', 'auth', 'author'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['Fantasy', 'fantasi', 'fantasy', 'fantasi'],\n",
       " ['popular', 'popular', 'popul', 'popular'],\n",
       " ['prosperity', 'prosper', 'prosp', 'prosper'],\n",
       " ['liberalization', 'liber', 'lib', 'liber'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['I', 'I', 'i', 'i'],\n",
       " ['will', 'will', 'wil', 'will'],\n",
       " ['deplore', 'deplor', 'depl', 'deplor'],\n",
       " ['democracy', 'democraci', 'democr', 'democraci'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['rarely', 'rare', 'rar', 'rare'],\n",
       " ['ever', 'ever', 'ev', 'ever'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Mann', 'mann', 'man', 'mann'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Mann', 'mann', 'man', 'mann'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['have', 'have', 'hav', 'have'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['accomplished', 'accomplish', 'accompl', 'accomplish'],\n",
       " ['Over', 'over', 'ov', 'over'],\n",
       " ['office', 'offic', 'off', 'offic'],\n",
       " ['has', 'ha', 'has', 'has'],\n",
       " ['never', 'never', 'nev', 'never'],\n",
       " ['has', 'ha', 'has', 'has'],\n",
       " ['statements', 'statement', 'stat', 'statement'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Mann', 'mann', 'man', 'mann'],\n",
       " ['He', 'He', 'he', 'he'],\n",
       " ['dignity', 'digniti', 'dign', 'digniti'],\n",
       " ['He', 'He', 'he', 'he'],\n",
       " ['So', 'So', 'so', 'so'],\n",
       " ['I', 'I', 'i', 'i'],\n",
       " ['probably', 'probabl', 'prob', 'probabl'],\n",
       " ['jealous', 'jealou', 'jeal', 'jealous'],\n",
       " ['Obama', 'obama', 'obam', 'obama'],\n",
       " ['belief', 'belief', 'believ', 'belief'],\n",
       " ['was', 'wa', 'was', 'was'],\n",
       " ['engagement', 'engag', 'eng', 'engag'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['make', 'make', 'mak', 'make'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['more', 'more', 'mor', 'more'],\n",
       " ['like', 'like', 'lik', 'like'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Mann', 'mann', 'man', 'mann'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['has', 'ha', 'has', 'has'],\n",
       " ['gone', 'gone', 'gon', 'gone'],\n",
       " ['directionShi', 'directionshi', 'directionsh', 'directionshi'],\n",
       " ['professor', 'professor', 'profess', 'professor'],\n",
       " ['relations', 'relat', 'rel', 'relat'],\n",
       " ['Beijing', 'beij', 'beid', 'beij'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['likely', 'like', 'lik', 'like'],\n",
       " ['care', 'care', 'car', 'care'],\n",
       " ['his', 'hi', 'his', 'his'],\n",
       " ['potential', 'potenti', 'pot', 'potenti'],\n",
       " ['ruler', 'ruler', 'rul', 'ruler'],\n",
       " ['perpetuityWith', 'perpetuitywith', 'perpetuitywi', 'perpetuitywith'],\n",
       " ['office', 'offic', 'off', 'offic'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['certainly', 'certainli', 'certain', 'certain'],\n",
       " ['office', 'offic', 'off', 'offic'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['White', 'white', 'whit', 'white'],\n",
       " ['This', 'thi', 'thi', 'this'],\n",
       " ['makes', 'make', 'mak', 'make'],\n",
       " ['has', 'ha', 'has', 'has'],\n",
       " ['like', 'like', 'lik', 'like'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['saidAt', 'saidat', 'said', 'saidat'],\n",
       " ['home', 'home', 'hom', 'home'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['will', 'will', 'wil', 'will'],\n",
       " ['probably', 'probabl', 'prob', 'probabl'],\n",
       " ['have', 'have', 'hav', 'have'],\n",
       " ['considerable', 'consider', 'consid', 'consider'],\n",
       " ['rivals', 'rival', 'riv', 'rival'],\n",
       " ['dissent', 'dissent', 'diss', 'dissent'],\n",
       " ['nationalists', 'nationalist', 'nat', 'nationalist'],\n",
       " ['cheered', 'cheer', 'che', 'cheer'],\n",
       " ['decision', 'decis', 'decid', 'decis'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['singular', 'singular', 'singul', 'singular'],\n",
       " ['restore', 'restor', 'rest', 'restor'],\n",
       " ['glory', 'glori', 'glory', 'glori'],\n",
       " ['news', 'news', 'new', 'news'],\n",
       " ['wrote', 'wrote', 'wrot', 'wrote'],\n",
       " ['totalitarian', 'totalitarian', 'totalit', 'totalitarian'],\n",
       " ['rule', 'rule', 'rul', 'rule'],\n",
       " ['passages', 'passag', 'pass', 'passag'],\n",
       " ['after', 'after', 'aft', 'after'],\n",
       " ['were', 'were', 'wer', 'were'],\n",
       " ['social', 'social', 'soc', 'social'],\n",
       " ['media', 'media', 'med', 'media'],\n",
       " ['legal', 'legal', 'leg', 'legal'],\n",
       " ['H', 'H', 'h', 'h'],\n",
       " ['China', 'china', 'chin', 'china'],\n",
       " ['Carnegie', 'carnegi', 'carnegy', 'carnegi'],\n",
       " ['sudden', 'sudden', 'sud', 'sudden'],\n",
       " ['move', 'move', 'mov', 'move'],\n",
       " ['before', 'befor', 'bef', 'befor'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['even', 'even', 'ev', 'even'],\n",
       " ['his', 'hi', 'his', 'his'],\n",
       " ['month', 'month', 'mon', 'month'],\n",
       " ['were', 'were', 'wer', 'were'],\n",
       " ['normal', 'normal', 'norm', 'normal'],\n",
       " ['Communist', 'communist', 'commun', 'communist'],\n",
       " ['Party', 'parti', 'party', 'parti'],\n",
       " ['This', 'thi', 'thi', 'this'],\n",
       " ['like', 'like', 'lik', 'like'],\n",
       " ['normal', 'normal', 'norm', 'normal'],\n",
       " ['order', 'order', 'ord', 'order'],\n",
       " ['going', 'go', 'going', 'go'],\n",
       " ['Mr', 'Mr', 'mr', 'mr'],\n",
       " ['Xi', 'Xi', 'xi', 'xi'],\n",
       " ['will', 'will', 'wil', 'will'],\n",
       " ['take', 'take', 'tak', 'take'],\n",
       " ['These', 'these', 'thes', 'these'],\n",
       " ['are', 'are', 'ar', 'are'],\n",
       " ['ordinary', 'ordinari', 'ordin', 'ordinari'],\n",
       " ['times', 'time', 'tim', 'time'],\n",
       " ['A', 'A', 'a', 'a'],\n",
       " ['series', 'seri', 'sery', 'seri'],\n",
       " ['senior', 'senior', 'seny', 'senior'],\n",
       " ['officials', 'offici', 'off', 'offici'],\n",
       " ['month', 'month', 'mon', 'month'],\n",
       " ['try', 'tri', 'try', 'tri'],\n",
       " ['administration', 'administr', 'admin', 'administr'],\n",
       " ...]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "different_stemmer_origin_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 Provide a count of unique Lemmas in your text using the NLTK WordNet lemmatizer. Provide a count of unique Lemmas in your text using the SpaCy lemmatizer. Provide a few examples where the lemmatizers gave different lemmas (if any), please do also include the original word/token from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the NLTK WordNet lemmatizer for puntuation-removed NLTK tokentized text\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemma_nltk = []\n",
    "d1 = {}\n",
    "for i in removedtext_nltk:\n",
    "    l = lemmatizer.lemmatize(i)\n",
    "    if l not in lemma_nltk:\n",
    "        lemma_nltk.append(l)\n",
    "        d1[l] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1704 unique Lemmas in my text using the NLTK WordNet lemmatizer\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", len(lemma_nltk), \"unique Lemmas in my text using the NLTK WordNet lemmatizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_spacy = []\n",
    "d2 = {}\n",
    "for token in tokens_spacy:\n",
    "    s = token.lemma_\n",
    "    if s not in lemma_spacy:\n",
    "        lemma_spacy.append(s)\n",
    "        d2[s] = str(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1837 unique Lemmas in my text using the SpaCy lemmatizer\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", len(lemma_spacy), \"unique Lemmas in my text using the SpaCy lemmatizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For word briefing ; the NLTK lemmatizer lemmatizes it to briefing ; the Spacy lemmatizer lemmatizes it to brief\n",
      "For word depicted ; the NLTK lemmatizer lemmatizes it to depicted ; the Spacy lemmatizer lemmatizes it to depict\n",
      "For word delivered ; the NLTK lemmatizer lemmatizes it to delivered ; the Spacy lemmatizer lemmatizes it to deliver\n",
      "For word has ; the NLTK lemmatizer lemmatizes it to ha ; the Spacy lemmatizer lemmatizes it to have\n",
      "For word been ; the NLTK lemmatizer lemmatizes it to been ; the Spacy lemmatizer lemmatizes it to be\n",
      "For word stunning ; the NLTK lemmatizer lemmatizes it to stunning ; the Spacy lemmatizer lemmatizes it to stun\n",
      "For word its ; the NLTK lemmatizer lemmatizes it to it ; the Spacy lemmatizer lemmatizes it to its\n",
      "For word referred ; the NLTK lemmatizer lemmatizes it to referred ; the Spacy lemmatizer lemmatizes it to refer\n",
      "For word said ; the NLTK lemmatizer lemmatizes it to said ; the Spacy lemmatizer lemmatizes it to say\n",
      "For word doused ; the NLTK lemmatizer lemmatizes it to doused ; the Spacy lemmatizer lemmatizes it to douse\n"
     ]
    }
   ],
   "source": [
    "# Provide 10 examples \n",
    "num = 0\n",
    "for k1, v1 in d1.items():\n",
    "    for k2, v2 in d2.items():\n",
    "        if v1 == v2 and k1!=k2:\n",
    "            num+=1\n",
    "            if num < 11: \n",
    "                print('For word', v1,'; the NLTK lemmatizer lemmatizes it to', k1,\\\n",
    "                      '; the Spacy lemmatizer lemmatizes it to', k2)\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5 In 3 to 5 sentences, comment on what you found when using the various tokenizers, stemmers and lemmatizers on your text(s)/corpus – specifically describe similarities and differences you detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Spacy tikenizer combines punctuations with words, so the number of tokens using NLTK is quite large. \n",
    "\n",
    "\n",
    "(2) The results of using Porter and Snowball stemmers are similar, except that Porter does not change caps to lowercases. \n",
    "\n",
    "\n",
    "(3) As for lemmatizers, SpaCy lemmatizer is more accurate. For example, for words: 'referred', 'has' and 'doused', NLTK cannot recognize the root verb but the Spacy lemmatizer could."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
