{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim \n",
    "import logging\n",
    "import glob, os\n",
    "from gensim.models import word2vec, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from ast import literal_eval\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use one of the larger SpaCy models containing vectors (I prefer “en_core_web_lg” but you can use any listed here under “Available models”: https://spacy.io/usage/models )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Choose 2 words/terms/tokens and provide the top 50 similarity scores for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar50(word):\n",
    "    by_similarity = sorted(word.vocab, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return [str(w.orth_) + \" : \" + str(w.similarity(word)) for w in by_similarity[:50]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lexeme.Lexeme object at 0x0000014E06338360>\n"
     ]
    }
   ],
   "source": [
    "similar1 = most_similar50(nlp.vocab[u'american'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['american : 1.0', 'American : 0.99999994', 'AMERICAN : 0.99999994', 'AMerican : 0.99999994', 'AMerica : 0.83003515', 'america : 0.83003515', 'America : 0.83003515', 'AMERICA : 0.83003515', 'British : 0.73649323', 'british : 0.73649323', 'BRITISH : 0.73649323', 'Canadian : 0.7126985', 'CANADIAN : 0.7126985', 'canadian : 0.7126985', 'EUropean : 0.71116835', 'EUROPEAN : 0.71116835', 'European : 0.71116835', 'european : 0.71116835', 'Americans : 0.7083187', 'americans : 0.7083187', 'AMERICANS : 0.7083187', 'AMericans : 0.7083187', 'African : 0.70086735', 'african : 0.70086735', 'AFRICAN : 0.70086735', 'Australian : 0.69194996', 'australian : 0.69194996', 'AUSTRALIAN : 0.69194996', 'UsA : 0.67251575', 'usa : 0.67251575', 'USa : 0.67251575', 'Usa : 0.67251575', 'USA : 0.67251575', 'MEXICAN : 0.6590071', 'Mexican : 0.6590071', 'mexican : 0.6590071', 'Indian : 0.64163625', 'indian : 0.64163625', 'INDIAN : 0.64163625', 'CHICAGO : 0.63874257', 'chicago : 0.63874257', 'CHicago : 0.63874257', 'Chicago : 0.63874257', 'CANADA : 0.63817865', 'canada : 0.63817865', 'Canada : 0.63817865', 'GERMAN : 0.6253866', 'german : 0.6253866', 'German : 0.6253866', 'irish : 0.6196904']\n"
     ]
    }
   ],
   "source": [
    "print(similar1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar2 = most_similar50(nlp.vocab[u'great'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great : 1.0', 'GReat : 1.0', 'GREAT : 1.0', 'greAt : 1.0', 'Great : 1.0', 'good : 0.84167075', 'Good : 0.84167075', 'gOOD : 0.84167075', 'GOOD : 0.84167075', 'GOod : 0.84167075', 'FANTASTIC : 0.8124874', 'Fantastic : 0.8124874', 'FANtastic : 0.8124874', 'fantastic : 0.8124874', 'Wonderful : 0.805945', 'wonderful : 0.805945', 'WONDERFUL : 0.805945', 'amazing : 0.78232324', 'AMazing : 0.78232324', 'AMAZING : 0.78232324', 'Amazing : 0.78232324', 'amaZing : 0.78232324', 'EXCELLENT : 0.77073884', 'Excellent : 0.77073884', 'excellent : 0.77073884', 'awesome : 0.7549282', 'Awesome : 0.7549282', 'AWesome : 0.7549282', 'AWEsome : 0.7549282', 'AWESOME : 0.7549282', 'VERy : 0.7500455', 'VERY : 0.7500455', 'Very : 0.7500455', 'very : 0.7500455', 'VEry : 0.7500455', 'Terrific : 0.7480562', 'terrific : 0.7480562', 'TERRIFIC : 0.7480562', 'nICE : 0.73561585', 'NICE : 0.73561585', 'Nice : 0.73561585', 'nice : 0.73561585', 'NIce : 0.73561585', 'really : 0.7232506', 'reAlly : 0.7232506', 'REALLy : 0.7232506', 'REALLY : 0.7232506', 'REally : 0.7232506', 'Really : 0.7232506', 'Well : 0.7213617']\n"
     ]
    }
   ],
   "source": [
    "print(similar2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Add 3 words/terms/tokens to the 2 you chose for #1 and provide the 1-to-1 comparative similarity scores for all 5. (E.g. for “dog”, “cat”, “worm”, “tree” and “banana” provide dog-cat, dog-worm, dog-tree, dog-banana, cat-worm, cat-tree, cat-banana, worm-tree, worm-banana and tree-banana)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "american great 0.27806786\n",
      "american beautiful 0.22853203\n",
      "american strong 0.20254456\n",
      "american make 0.30104175\n",
      "great beautiful 0.59625214\n",
      "great strong 0.53746843\n",
      "great make 0.5798683\n",
      "beautiful strong 0.3536118\n",
      "beautiful make 0.4160449\n",
      "strong make 0.47789517\n"
     ]
    }
   ],
   "source": [
    "mytoken = nlp(\"american great beautiful strong make\")\n",
    "for n in range(len(mytoken)):\n",
    "    for m in range(n+1,len(mytoken)):\n",
    "        print(mytoken[n].text, mytoken[m].text, mytoken[n].similarity(mytoken[m]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using your corpus and the gensim package, train a NN (neural network) Word2Vec model, then provide the code (especially your parameters) and the following items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speaker</th>\n",
       "      <th>FileName</th>\n",
       "      <th>RawText</th>\n",
       "      <th>Token</th>\n",
       "      <th>Cleaned_text</th>\n",
       "      <th>Sent_Token</th>\n",
       "      <th>Normalized_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BUSH</td>\n",
       "      <td>../Project/Data/BUSH/BUSH-0.txt</td>\n",
       "      <td>thank you very much. i want to thank arizona ...</td>\n",
       "      <td>['thank', 'much', 'want', 'thank', 'arizona', ...</td>\n",
       "      <td>thank much want thank arizona state well yes s...</td>\n",
       "      <td>[' thank you very much.', 'i want to thank ari...</td>\n",
       "      <td>[['thank', 'much'], ['want', 'thank', 'arizona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BUSH</td>\n",
       "      <td>../Project/Data/BUSH/BUSH-1.txt</td>\n",
       "      <td>gosh, i just do not think i ever said i am no...</td>\n",
       "      <td>['gosh', 'think', 'ever', 'said', 'worried', '...</td>\n",
       "      <td>gosh think ever said worried osama bin laden k...</td>\n",
       "      <td>[' gosh, i just do not think i ever said i am ...</td>\n",
       "      <td>[['gosh', 'think', 'ever', 'say', 'worry', 'os...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BUSH</td>\n",
       "      <td>../Project/Data/BUSH/BUSH-10.txt</td>\n",
       "      <td>i think it is important to promote a culture ...</td>\n",
       "      <td>['think', 'important', 'promote', 'culture', '...</td>\n",
       "      <td>think important promote culture life think hos...</td>\n",
       "      <td>[' i think it is important to promote a cultur...</td>\n",
       "      <td>[['think', 'important', 'promote', 'culture', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BUSH</td>\n",
       "      <td>../Project/Data/BUSH/BUSH-11.txt</td>\n",
       "      <td>gosh, i sure hope it is not the administratio...</td>\n",
       "      <td>['gosh', 'sure', 'hope', 'administration', 'lo...</td>\n",
       "      <td>gosh sure hope administration look systemic pr...</td>\n",
       "      <td>[' gosh, i sure hope it is not the administrat...</td>\n",
       "      <td>[['gosh', 'sure', 'hope', 'administration'], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BUSH</td>\n",
       "      <td>../Project/Data/BUSH/BUSH-12.txt</td>\n",
       "      <td>i think it is important, since he talked abou...</td>\n",
       "      <td>['think', 'important', 'since', 'talked', 'med...</td>\n",
       "      <td>think important since talked medicare plan uni...</td>\n",
       "      <td>[' i think it is important, since he talked ab...</td>\n",
       "      <td>[['think', 'important', 'since', 'talk', 'medi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Speaker                          FileName  \\\n",
       "0    BUSH   ../Project/Data/BUSH/BUSH-0.txt   \n",
       "1    BUSH   ../Project/Data/BUSH/BUSH-1.txt   \n",
       "2    BUSH  ../Project/Data/BUSH/BUSH-10.txt   \n",
       "3    BUSH  ../Project/Data/BUSH/BUSH-11.txt   \n",
       "4    BUSH  ../Project/Data/BUSH/BUSH-12.txt   \n",
       "\n",
       "                                             RawText  \\\n",
       "0   thank you very much. i want to thank arizona ...   \n",
       "1   gosh, i just do not think i ever said i am no...   \n",
       "2   i think it is important to promote a culture ...   \n",
       "3   gosh, i sure hope it is not the administratio...   \n",
       "4   i think it is important, since he talked abou...   \n",
       "\n",
       "                                               Token  \\\n",
       "0  ['thank', 'much', 'want', 'thank', 'arizona', ...   \n",
       "1  ['gosh', 'think', 'ever', 'said', 'worried', '...   \n",
       "2  ['think', 'important', 'promote', 'culture', '...   \n",
       "3  ['gosh', 'sure', 'hope', 'administration', 'lo...   \n",
       "4  ['think', 'important', 'since', 'talked', 'med...   \n",
       "\n",
       "                                        Cleaned_text  \\\n",
       "0  thank much want thank arizona state well yes s...   \n",
       "1  gosh think ever said worried osama bin laden k...   \n",
       "2  think important promote culture life think hos...   \n",
       "3  gosh sure hope administration look systemic pr...   \n",
       "4  think important since talked medicare plan uni...   \n",
       "\n",
       "                                          Sent_Token  \\\n",
       "0  [' thank you very much.', 'i want to thank ari...   \n",
       "1  [' gosh, i just do not think i ever said i am ...   \n",
       "2  [' i think it is important to promote a cultur...   \n",
       "3  [' gosh, i sure hope it is not the administrat...   \n",
       "4  [' i think it is important, since he talked ab...   \n",
       "\n",
       "                                     Normalized_list  \n",
       "0  [['thank', 'much'], ['want', 'thank', 'arizona...  \n",
       "1  [['gosh', 'think', 'ever', 'say', 'worry', 'os...  \n",
       "2  [['think', 'important', 'promote', 'culture', ...  \n",
       "3  [['gosh', 'sure', 'hope', 'administration'], [...  \n",
       "4  [['think', 'important', 'since', 'talk', 'medi...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read cleaned data\n",
    "df = pd.read_csv(\"../Project/df.csv\")\n",
    "df = df.drop([\"Unnamed: 0\"],axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BUSH', 'CRUZ', 'HILLARY', 'KERRY', 'MCCAIN', 'OBAMA', 'ROMNEY',\n",
       "       'SANDERS', 'TRUMP'], dtype=object)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show speaker's name\n",
    "df[\"Speaker\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group speakers' into two parties\n",
    "Rep = [\"TRUMP\",\"CRUZ\",\"ROMNEY\",\"BUSH\",\"MCCAIN\"]\n",
    "Dem = [\"HILLARY\",\"OBAMA\",\"SANDERS\",\"KERRY\"]\n",
    "Rep_df = df.loc[df['Speaker'].isin(Rep)][['Speaker','Normalized_list']]\n",
    "Dem_df = df.loc[df['Speaker'].isin(Dem)][['Speaker','Normalized_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Normalized_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BUSH</td>\n",
       "      <td>[['thank', 'much'], ['want', 'thank', 'arizona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BUSH</td>\n",
       "      <td>[['gosh', 'think', 'ever', 'say', 'worry', 'os...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BUSH</td>\n",
       "      <td>[['think', 'important', 'promote', 'culture', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BUSH</td>\n",
       "      <td>[['gosh', 'sure', 'hope', 'administration'], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BUSH</td>\n",
       "      <td>[['think', 'important', 'since', 'talk', 'medi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Speaker                                    Normalized_list\n",
       "0    BUSH  [['thank', 'much'], ['want', 'thank', 'arizona...\n",
       "1    BUSH  [['gosh', 'think', 'ever', 'say', 'worry', 'os...\n",
       "2    BUSH  [['think', 'important', 'promote', 'culture', ...\n",
       "3    BUSH  [['gosh', 'sure', 'hope', 'administration'], [...\n",
       "4    BUSH  [['think', 'important', 'since', 'talk', 'medi..."
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rep_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Choose 5 words/terms/tokens and provide the 10 most similar words/terms/tokens from your corpus according to your model, plus the similiarity score for each match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data and covert string to list\n",
    "\n",
    "def remove_punctuation(l):\n",
    "    r = []\n",
    "    not_wanted = [\"‘\",\"’\",'”','“','–']\n",
    "    for i in l:\n",
    "        removed = [x for x in i if x not in not_wanted]\n",
    "        r.append(removed)\n",
    "    return r\n",
    "\n",
    "Rep_df.Normalized_list = Rep_df.Normalized_list.apply(literal_eval)\n",
    "Dem_df.Normalized_list = Dem_df.Normalized_list.apply(literal_eval)\n",
    "\n",
    "Rep_sent = Rep_df.Normalized_list.sum()\n",
    "Dem_sent = Dem_df.Normalized_list.sum()\n",
    "\n",
    "Dem_sent = remove_punctuation(Dem_sent)\n",
    "Rep_sent = remove_punctuation(Rep_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rep_trigram_sentences_project = []\n",
    "Dem_trigram_sentences_project = []\n",
    "\n",
    "# Set values for various parameters                                   [= \"Tuning the Model\"]\n",
    "num_features = 150    # Word vector dimensionality                      \n",
    "min_word_count = 1    # Minimum word count                        \n",
    "num_workers = 20      # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "Dem_bigram = Phraser(Phrases(Dem_sent))\n",
    "Dem_trigram = Phraser(Phrases(Dem_bigram[Dem_sent]))\n",
    "Rep_bigram = Phraser(Phrases(Rep_sent))\n",
    "Rep_trigram = Phraser(Phrases(Rep_bigram[Rep_sent]))\n",
    "\n",
    "for sent in Rep_sent:\n",
    "    bigrams_ = Rep_bigram[sent]\n",
    "    trigrams_ = Rep_trigram[Rep_bigram[sent]]\n",
    "    Rep_trigram_sentences_project.append(trigrams_)\n",
    "    \n",
    "for sent in Dem_sent:\n",
    "    bigrams_ = Dem_bigram[sent]\n",
    "    trigrams_ = Dem_trigram[Dem_bigram[sent]]\n",
    "    Dem_trigram_sentences_project.append(trigrams_)\n",
    "\n",
    "Rep_model = word2vec.Word2Vec(Rep_trigram_sentences_project, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling, sg=1)           #sg=1 turns on skip-grams, otherwise CBOW\n",
    "\n",
    "Dem_model = word2vec.Word2Vec(Dem_trigram_sentences_project, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dem_vocab = list(Dem_model.wv.vocab.keys())[:5]\n",
    "Rep_vocab = list(Rep_model.wv.vocab.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_similar_result(party,l,model):\n",
    "    print('For',party,\", the 10 most similar words/terms/tokens for the following words are:\" )\n",
    "    print(\"\")\n",
    "    for w in l:\n",
    "        print(w)\n",
    "        r = model.wv.most_similar (positive=w)\n",
    "        print(r)\n",
    "        print(\"\")\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Republican party , the 10 most similar words/terms/tokens for the following words are:\n",
      "\n",
      "thank_much\n",
      "[('leadership', 0.9982559680938721), ('market', 0.9982283711433411), ('constitutional', 0.9982229471206665), ('longterm', 0.9982197284698486), ('thought', 0.9982196092605591), ('risk', 0.9982132911682129), ('behind', 0.998207688331604), ('around', 0.9982069730758667), ('sign', 0.9981980919837952), ('wife', 0.9981955289840698)]\n",
      "\n",
      "want_thank\n",
      "[('guy', 0.9997182488441467), ('medicare', 0.9997088313102722), ('head', 0.999699592590332), ('technology', 0.9996991753578186), ('pretty', 0.9996957182884216), ('citizen', 0.9996956586837769), ('succeed', 0.9996895790100098), ('account', 0.9996885061264038), ('iraq', 0.9996881484985352), ('united_state_america', 0.999687910079956)]\n",
      "\n",
      "arizona\n",
      "[('poverty', 0.9991181492805481), ('founding', 0.9991021156311035), ('standing', 0.9990628957748413), ('abortion', 0.9990487098693848), ('daughter', 0.9990425109863281), ('remain', 0.9990195035934448), ('call', 0.999015748500824), ('jan_20_2017', 0.9990126490592957), ('member', 0.9990085363388062), ('elect', 0.999006986618042)]\n",
      "\n",
      "state\n",
      "[('new_hampshire', 0.999396800994873), ('wisconsin', 0.9993851780891418), ('sir_barack_obama', 0.9993471503257751), ('colorado', 0.9993413686752319), ('week_ago', 0.999340295791626), ('victory', 0.9993390440940857), ('medium', 0.9993265867233276), ('little_old_man', 0.9993159770965576), ('across_country', 0.9993125200271606), ('four', 0.9993067979812622)]\n",
      "\n",
      "well\n",
      "[('create', 0.9996432662010193), ('technology', 0.9996360540390015), ('raise', 0.9996127486228943), ('joe_plumber', 0.9995965957641602), ('happen', 0.9995880126953125), ('source', 0.9995856881141663), ('price', 0.9995770454406738), ('whole', 0.9995764493942261), ('senator', 0.9995759725570679), ('politics', 0.9995682239532471)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Rep_r = return_similar_result(\"Republican party\", Rep_vocab, Rep_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Democratic party , the 10 most similar words/terms/tokens for the following words are:\n",
      "\n",
      "know\n",
      "[('away', 0.9997724890708923), ('government', 0.9997686743736267), ('far', 0.9997611045837402), ('company', 0.9997572302818298), ('oil', 0.999756932258606), ('fact', 0.9997559189796448), ('since', 0.9997555613517761), ('would', 0.9997535943984985), ('people', 0.9997533559799194), ('order', 0.9997531175613403)]\n",
      "\n",
      "take\n",
      "[('process', 0.9997642040252686), ('never', 0.9997590780258179), ('go', 0.9997581839561462), ('war', 0.999754786491394), ('support', 0.9997515082359314), ('country', 0.9997515082359314), ('care', 0.9997506737709045), ('leadership', 0.999748945236206), ('like', 0.9997448325157166), ('run', 0.9997419118881226)]\n",
      "\n",
      "get\n",
      "[('go', 0.9997610449790955), ('mean', 0.9997559189796448), ('provide', 0.9997465014457703), ('special', 0.9997453093528748), ('see', 0.9997438192367554), ('political', 0.9997432827949524), ('billion', 0.9997409582138062), ('trade_agreement', 0.9997403621673584), ('effort', 0.9997400045394897), ('good', 0.9997377991676331)]\n",
      "\n",
      "thing\n",
      "[('teacher', 0.9997761249542236), ('run', 0.9997689723968506), ('reach', 0.9997658729553223), ('small_business', 0.9997649788856506), ('american', 0.9997621774673462), ('kind', 0.999761700630188), ('something', 0.9997562170028687), ('large', 0.9997552633285522), ('long', 0.999754011631012), ('strong', 0.9997537136077881)]\n",
      "\n",
      "find\n",
      "[('three', 0.9997689127922058), ('something', 0.9997535943984985), ('try', 0.9997534155845642), ('strong', 0.9997527599334717), ('oil', 0.9997495412826538), ('bring', 0.999749481678009), ('fight', 0.9997451901435852), ('agree', 0.9997447729110718), ('member', 0.9997425079345703), ('friend', 0.9997403025627136)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Dem_r = return_similar_result(\"Democratic party\", Dem_vocab, Dem_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Provide the relative cosine similarity for two word pairs, based on your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Republican party , the relative cosine similarity for two word pairs are:\n",
      "\n",
      "thank_much create 0.09998488592118782\n",
      "thank_much technology 0.09998999719309964\n",
      "thank_much raise 0.09997955968924231\n",
      "thank_much joe_plumber 0.09997454395512323\n",
      "thank_much happen 0.09997299743710317\n",
      "thank_much source 0.09998459930780959\n",
      "thank_much price 0.09998461125003368\n",
      "thank_much whole 0.0999909645132512\n",
      "thank_much senator 0.09997740411779352\n",
      "thank_much politics 0.09999206319786776\n",
      "want_thank create 0.09999269741142314\n",
      "want_thank technology 0.10000020390964184\n",
      "want_thank raise 0.09998937046463519\n",
      "want_thank joe_plumber 0.09998859536950538\n",
      "want_thank happen 0.09998630585773732\n",
      "want_thank source 0.09999666828339586\n",
      "want_thank price 0.09999713334047375\n",
      "want_thank whole 0.09999642383031646\n",
      "want_thank senator 0.09999367522374075\n",
      "want_thank politics 0.09999843907765396\n",
      "arizona create 0.09998532143331518\n",
      "arizona technology 0.09997625882192865\n",
      "arizona raise 0.09997587102091475\n",
      "arizona joe_plumber 0.09998023229077883\n",
      "arizona happen 0.09997513718207304\n",
      "arizona source 0.09998006523803438\n",
      "arizona price 0.09998869231905144\n",
      "arizona whole 0.09996957671215058\n",
      "arizona senator 0.09997757734537593\n",
      "arizona politics 0.09998729026923192\n",
      "state create 0.09997270275803777\n",
      "state technology 0.09997210035424504\n",
      "state raise 0.09997124148151085\n",
      "state joe_plumber 0.09998067118840497\n",
      "state happen 0.09997896537172456\n",
      "state source 0.09997663925806947\n",
      "state price 0.09997075240120389\n",
      "state whole 0.09997053171862635\n",
      "state senator 0.09998191774674833\n",
      "state politics 0.09997922780506001\n",
      "well create 0.10000472796235418\n",
      "well technology 0.1000040064546674\n",
      "well raise 0.10000166900827713\n",
      "well joe_plumber 0.10000007095819399\n",
      "well happen 0.09999918249005076\n",
      "well source 0.09999896782660675\n",
      "well price 0.09999811513570418\n",
      "well whole 0.09999804358122284\n",
      "well senator 0.09999799587823528\n",
      "well politics 0.0999972207046875\n"
     ]
    }
   ],
   "source": [
    "print('For',\"Republican party\",\", the relative cosine similarity for two word pairs are:\" )\n",
    "print(\"\")\n",
    "for n in Rep_vocab:\n",
    "    for m in Rep_r:\n",
    "        print(n,m[0],Rep_model.wv.relative_cosine_similarity(n, m[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Democratic party , the relative cosine similarity for two word pairs are:\n",
      "\n",
      "know three 0.09999691590783621\n",
      "know something 0.09999631971758612\n",
      "know try 0.0999944894135183\n",
      "know strong 0.09999704110778873\n",
      "know oil 0.09999981339245172\n",
      "know bring 0.09999823945019144\n",
      "know fight 0.09999903238322408\n",
      "know agree 0.09999425093741825\n",
      "know member 0.09998860501574977\n",
      "know friend 0.0999950617561584\n",
      "take three 0.09998683782576717\n",
      "take something 0.09999498183537085\n",
      "take try 0.0999882985009889\n",
      "take strong 0.09999064750522306\n",
      "take oil 0.09999464796674874\n",
      "take bring 0.09999615633748793\n",
      "take fight 0.09999709832395746\n",
      "take agree 0.09999162526333068\n",
      "take member 0.09999589997408166\n",
      "take friend 0.09999494010179308\n",
      "get three 0.09999200975198978\n",
      "get something 0.09999654682014064\n",
      "get try 0.09999415010345648\n",
      "get strong 0.09999887795502498\n",
      "get oil 0.09999712513237537\n",
      "get bring 0.09999710724642996\n",
      "get fight 0.09999180704460854\n",
      "get agree 0.09999367910689418\n",
      "get member 0.09998824177949131\n",
      "get friend 0.0999933810078041\n",
      "thing three 0.09999329049576607\n",
      "thing something 0.09999943123626806\n",
      "thing try 0.09999144827361546\n",
      "thing strong 0.0999991808371408\n",
      "thing oil 0.09999842367787502\n",
      "thing bring 0.09999501348023701\n",
      "thing fight 0.09999261680287604\n",
      "thing agree 0.09999140654042758\n",
      "thing member 0.0999899697263878\n",
      "thing friend 0.09999564543993916\n",
      "find three 0.10000189888256573\n",
      "find something 0.10000035473630349\n",
      "find try 0.10000033685043945\n",
      "find strong 0.10000027126893796\n",
      "find oil 0.09999994932338521\n",
      "find bring 0.09999994932338521\n",
      "find fight 0.09999952006264821\n",
      "find agree 0.09999948429092013\n",
      "find member 0.09999925773664228\n",
      "find friend 0.09999903118236442\n"
     ]
    }
   ],
   "source": [
    "print('For',\"Democratic party\",\", the relative cosine similarity for two word pairs are:\" )\n",
    "print(\"\")\n",
    "for n in Dem_vocab:\n",
    "    for m in Dem_r:\n",
    "        print(n,m[0],Dem_model.wv.relative_cosine_similarity(n, m[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c.Provide an example of an analogy from your model. (Don't worry if it seems to make no sense.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'also', 'israel', 'record', 'mass', 'discuss', 'single', 'fill', 'add', 'fund', 'background', 'chris', 'v', 'bernie', 'average', 'arm', 'doctor', 'relation', 'mean', 'iranian', 'journey', 'campaigning', 'establish', 'everyone', 'stand', 'interest', 'officer', 'lead', 'dishonest', 'page', 'body', 'tuesday', 'quickly', 'nevada', 'movement', 'easier', 'majority', 'patriot', 'career', 'applaud', 'target', 'think', 'longterm', 'behind', 'hand', 'zero', 'month', 'three', 'government', 'reserve', 'primary', 'social_security', '8', 'third', 'take', 'burn', 'doubt', 'amazing', 'unbelievable', 'discrimination', 'goal', 'mayor', 'even', 'want_thank', 'insurance', 'apology', 'neither', '911', 'heal', 'lie', 'additional', 'wrong', 'oppose', 'adult', '2000', 'car', 'vote', 'lack', 'child', 'credit', 'gut', 'socialism', 'south', 'workforce', 'senate', 'payment', 'freeze', 'certain', 'trust', 'fifth', 'pose', 'assure', 'global', 'integrity', 'europe', 'vibrant', 'half', 'aside', 'thank_much', 'show', 'politician', 'memorial', 'obviously', 'economy', 'reason', 'rich', 'could', 'instate', 'choose', 'transportation', 'foot', 'essential', 'moment', 'opportunity', 'text', 'energy', 'nobody', 'note', 'overturn', 'minority', 'expensive', 'stuff', 'fear', 'good', 'area', 'disabled', '12000', 'company', 'training', 'nuclear', 'freedom', 'rent', 'reasonable', 'lease', 'break', 'prosperity', 'welcome', 'hundred', 'affordable', 'already', 'north', 'offer', 'enrich', 'unprecedented', 'somebody', 'scalpel', 'solar', 'price', 'sister', 'tough', 'combat', 'thousand', 'book', '20', 'deeply', 'world', 'anywhere', 'bit', 'mother', 'worry', 'truth', 'pledge', 'budget', 'consumer', 'casino', 'equipment', 'guarantee', 'properly', 'nafta', 'devote', 'one', '9', 'renegotiate', 'resort', 'owner', 'biodiesel', 'chance', 'difficulty', 'exception', 'project', 'jail', 'hillary_clinton', 'board', 'unless', 'opposite', 'clinton', 'premium', 'referendum', 'profound', 'invitation', 'tragic', 'twice', 'criminal', 'district', 'federal', 'probably', 'whether', 'whoever', 'judge', 'employer', 'society', 'saw', 'field', 'solvent', 'recall', 'soldier', 'compromise', 'block', 'new_hampshire', 'power', 'predict', 'deep', 'transparent', 'turn', 'rightly', 'suit', 'sector', 'give', 'ultimately', 'hug', 'entirely', 'hall', 'estate', '14', 'first', 'technology', 'corporate', 'accomplish', 'steward', 'wisconsin', 'maine', 'tired', 'begin', 'wind', 'excite', 'source', 'simple', 'va', 'billion', 'teaching', 'voucher', 'total', 'radical', 'get', 'outcome', 'explain', 'innovation', 'join', '2016', 'alone', 'marriage', 'regret', 'dedicate', 'committee', 'speech', 'assistance', 'sum', 'house', 'part', 'bust', 'resource', 'nail', 'emphasize', 'weapon', 'hometown', 'insure', 'hurt', 'john', 'afford', 'wish', 'lower', 'bottom', 'market', 'transaction', 'respond', 'state', 'somewhere', 'insult', 'move', 'within', 'create', 'remind', 'schumer', 'line', 'senior', 'health_care', 'door', 'street', 'wonder', 'dream', 'dick', 'believe', 'office', 'official', 'match', 'usual', 'ban', 'successful', 'full', 'suggest', 'danger', 'impress', 'farmer', 'work', 'travel', 'trillion', 'transform', 'loan', 'equal', 'surely', 'practice', 'trouble', 'partisan', 'prosecution', 'ad', 'husband', '1', 'exact', 'crazy', 'wonderful', 'pressure', 'community', 'ensure', 'consider', 'standing', 'progress', 'nation', 'capable', 'ayers', 'difficult', 'center', 'revolutionary', 'lifetime', 'paul', 'miss', 'forward', 'plant', 'parenthood', '2009', 'want', 'important', 'medicare', 'govern', 'floor', 'assemble', '6', 'test', 'address', 'statement', 'protection', 'network', 'rule', 'push', 'convention', 'organization', 'knowhow', 'driver', 'number', 'cap', 'maintain', 'stop', 'beyond', 'common', 'study', 'charge', 'determination', 'second', 'consumption', 'drill', 'big', 'agenda', 'personally', 'investing', 'end', 'fiscally', 'mostly', 'challenge', 'gay', 'increase', 'violate', 'existential', 'sorry', 'tear', 'failure', 'prosecute', 'admit', 'c', '43', 'stick', 'poverty', 'workplace', 'comment', 'immoral', 'november', 'bank', 'venezuela', 'basic', 'spending', 'father', 'home', 'defeat', 'religious', 'late', '18', 'terrorist', 'leadership', 'compete', 'critical', 'volunteer', 'drilling', 'follow', 'entire', 'charter', 'invest', 'purpose', 'court', 'burden', 'account', 'college', 'case', '15', 'capital', 'bear', 'go', 'think_important', 'cannon', 'dependence', 'commerce', 'bar', 'interesting', 'happen', 'commit', 'sense', 'handful', 'quick', '10', 'without', 'optimistic', 'demand', 'voting', 'finding', 'university', 'hatchet', 'bob', 'available', 'spend', 'ahead', 'powerful', 'iowa', 'lose', 'four', 'caucus', 'enormous', 'worse', 'unacceptable', 'godgiven', 'trade', 'reagan', 'tax_cut', 'nominate', 'thrill', 'invite', 'true', 'joe', 'privilege', 'publicly', 'death', 'black', 'great', 'dignity', 'medical', 'elect', 'almighty', 'defend', 'right', 'change', 'border', 'either', 'ground', 'police', 'incredible', 'appeal', 'brother', 'texas', 'tom', 'speaking', 'justice', 'legitimate', 'sander', 'present', 'loss', 'sound', 'much', 'special', 'torture', 'rubio', 'planet', 'plumber', 'shake', 'destroy', 'make_sure', 'former', 'difference', 'ideological', 'acknowledge', 'old', 'obey', 'manufacturing', 'landmark', 'amount', 'example', 'hell', 'hour', 'coal', 'port', 'actually', 'rush', 'necessary', 'conclude', 'otherwise', 'china', 'careful', 'may', 'union', 'mention', '2', '100', 'course', 'muslim', 'week', 'expand', 'convinced', 'remark', 'extend', 'host', 'system', 'ohio', 'release', 'accountable', '9000', 'sort', 'along', 'heartland', 'corrupt', 'gold', 'incompetent', 'american', 'especially', 'proposal', 'assault', 'everywhere', 'rip', 'would', 'competitive', 'reverse', 'plan', 'eye', 'law', 'leg', 'live', 'girl', 'propose', 'significant', 'likely', 'secretary', 'medicine', 'morning', 'hire', 'mom', 'environmental', 'attack', 'national', 'education', 'acrosstheboard', 'air', 'production', 'folk', 'rest', 'past', 'profit', 'boldly', 'people', 'hollow', 'mate', 'specific', 'regulation', 'public', 'abortion', 'lady', 'citizenship', 'leave', 'priority', 'adopt', 'buy', 'say', 'saving', 'mexican', 'completely', 'call', 'bed', 'behavior', 'lot', 'stark', 'afghanistan', 'legislate', '25', '7', 'empty', 'exist', 'pope', 'since', 'pastor', 'grandchild', 'status', 'independence', 'deal', 'restore', 'donald_trump', 'list', 'cent', 'appointment', 'chuck', 'elsewhere', 'remain', 'know', 'teacher', 'force', 'fought', 'ten', 'matter', 'brag', 'campaign', 'finish', 'undermine', 'judgment', 'dismiss', 'fail', 'achievement', '36', 'distant', 'unique', 'disease', 'really', 'realize', 'bright', 'legislation', 'sanction', 'check', 'grow', 'smallbusiness', 'direction', 'illinois', 'threaten', 'serve', 'health', 'murder', 'subsidy', 'vigorously', 'none', 'professional', 'ambassador', 'wage', 'general', 'story', 'catholic', 'yet', 'behalf', 'something', 'colleague', 'thirdly', 'way', 'leader', 'intelligence', 'hispanic', 'diagnose', 'heres', 'greedy', 'adoption', 'debt', 'surround', 'evening', 'place', 'vladimir', 'assad', 'several', 'core', '5', 'nearly', 'serious', 'bureaucracy', 'environment', 'private', 'describe', 'fair', 'bombing', 'hopefully', 'file', 'intend', 'secondly', 'industry', 'find', 'effort', 'prescription', 'executive', 'ought', 'council', 'opponent', 'remember', 'disagree', 'provision', 'express', 'century', 'acorn', 'motivate', 'preside', 'currently', 'control', 'isi', 'basis', 'truly', 'california', 'party', 'evidence', 'saddle', 'mr', 'pleasure', 'characterize', 'gas', 'establishment', 'feeling', 'piece', 'except', 'united_state', 'principle', '…the', 'appear', 'ally', 'solve', 'aggressive', 'together', 'development', 'veteran', 'debate', 'southern', 'violence', '500', 'quality', 'role', 'ceiling', '300', 'guy', 'future', 'everybody', 'love', 'percent', 'gasoline', 'conversation', 'cover', 'agree', 'food', 'across_country', 'key', 'goodpaying', 'game', 'marco', 'sign', 'positive', 'fiscal', 'prison', 'modern', 'educate', 'walk', 'waver', 'admire', 'compassionate', 'open', 'congressional', 'calm', 'systemic', 'tyranny', 'country', 'congress', 'program', 'nothing', 'briefly', 'anyone', 'thinking', 'autism', 'unfortunately', '200', 'generation', 'last', 'earmark', 'security', 'connect', 'refuse', 'straightforward', 'depression', 'worker', 'order', '4000', 'send', 'ever', 'shy', 'certainly', '13', 'deserve', 'threat', 'conviction', 'report', 'somehow', 'real', 'republican', 'spring', 'able', 'pursue', 'barrier', 'next', 'terror', 'highly', 'crisis', 'classroom', 'phone', 'base', 'administration', 'democratic', 'job', 'equality', 'veto', 'history', 'secure', 'iraq', 'whole', 'local', 'partner', 'immigration', 'disaster', 'away', 'bully', 'drug', 'forget', 'grant', 'guide', 'drop', 'candidate', 'speak', 'let', 'george', 'cause', 'tonight', 'us', 'dollar', 'political', 'county', 'association', 'outrageous', 'might', 'rebuild', 'blue', '98', 'visit', 'prime', 'accept', 'introduce', 'period', 'support', 'detail', 'proud', 'major', 'process', 'try', 'longer', 'ethnic', 'murderer', 'around', 'safety', 'level', 'infrastructure', 'table', 'goodbye', 'rid', 'adjustment', 'fine', 'question', 'towards', 'switch', 'young', 'firefighter', 'lift', 'agreement', 'represent', 'watch', 'measure', 'marry', 'continue', 'five', 'forth', 'gain', 'pass', 'trial', 'buffett', 'reality', 'camera', '200000', 'frankly', 'exactly', 'issue', 'judicial', 'yes', 'fall', 'determine', 'play', 'later', 'suicide', 'liberty', 'homeowner', 'article', 'idea', 'promote', 'reflect', 'embassy', 'independent', 'fully', 'high', 'environmentalist', 'run', 'dictator', 'bush', '3', 'cross', 'fact', 'risk', 'position', 'become', 'president_obama', 'enough', 'different', 'view', 'employee', 'expect', 'enjoy', 'lateterm', 'willing', 'protect', 'lewis', 'require', '17', 'india', 'blood', 'wealth', 'six', 'secret', 'overseas', 'listen', 'chamber', 'perpetrate', 'particularly', 'put', 'clean', 'arizona', 'attempt', 'midst', '11', 'building', 'fellow', 'front', 'baby', 'claim', 'capability', 'initiative', 'sometimes', 'result', 'among', 'strongly', 'human', 'thing', 'tit', 'comprehensive', 'defense', 'housing', 'kennedy', 'start', 'finance', 'childrens', 'manufacturer', 'hard', 'long', 'save', 'addition', 'soon', 'notice', 'men', 'else', 'exercise', 'build', 'investment', 'sick', 'path', 'capitalist', 'hunt', 'hole', 'fulfill', 'hear', 'absolutely', 'twothirds', 'rally', 'borrow', 'afghan', 'improve', 'exchange', 'brought', 'chairman', 'degree', '60000', 'subject', 'today', 'achieve', 'responsible', 'unemployed', 'best', 'unleash', 'grab', 'everything', 'carolina', 'lifting', 'schedule', 'sure', 'advance', 'unemployment', 'read', 'whatever', 'tv', 'peace', 'win', 'large', 'pain', 'consequence', 'throughout', 'patient', 'grateful', 'hero', 'video', 'language', 'citizen', 'action', 'suppose', 'socalled', 'american_people', 'stake', 'family', 'prioritize', 'responsibility', 'perspective', 'need', 'entrepreneur', 'act', 'choice', 'doma', 'kind', 'lawsuit', 'import', 'working', 'department', 'blessing', 'gun', 'favor', 'woman', 'though', 'condition', 'commitment', 'export', 'vietnam', 'concerned', 'range', 'hardworking', 'another', 'per', 'face', 'outside', 'excitement', 'hope', 'coverage', 'healthy', 'absurd', 'cast', 'sexual', 'coalition', 'glad', 'importantly', 'apart', 'smart', 'hey', 'bad', 'echo', 'crime', 'individual', 'bring', 'destruction', 'engage', 'sacrifice', 'taxpayer', 'contribute', 'implement', 'fuel', 'decision', 'middle', 'collapse', 'boy', 'due', 'vision', 'parent', 'military', 'angry', 'double', 'see', 'putin', 'hospital', 'fix', 'include', 'advantage', 'crack', 'strategy', 'new', 'benefit', 'extraordinary', 'dangerous', 'worth', 'learn', 'audio', 'maybe', 'strict', 'fly', 'task', 'ownership', 'thanks', 'pool', 'transparency', 'kerry', '50', 'supportive', 'carry', 'wade', 'wife', 'god', 'airplane', 'expense', 'foreign', 'policy', 'white_house', 'contributor', 'recent', 'spirit', 'efficient', 'money', 'mistake', 'africanamerican', 'penalty', 'bless', 'guess', 'homeland', 'pulpit', 'top', 'bother', 'demonstrate', 'die', 'cut', 'particular', 'differently', 'supreme', 'eliminate', 'tell', 'sen', '30000', 'often', 'tackle', 'imagine', 'fifty', 'speed', 'knowledge', 'quite', 'syria', 'free', 'economically', '40', 'pretty', 'fire', 'arab', '750', 'cheat', 'racial', 'town', 'promise', 'repeal', 'current', 'option', 'super', 'guard', 'anything', 'divide', 'reach', 'math', 'afraid', 'constitution', 'european', '1990s', 'day', 'use', '28', 'pick', 'relieve', 'culture', 'dramatic', 'possibly', 'bipartisan', 'team', 'mccain', 'pour', '…i', 'reduce', 'embrace', 'contribution', 'clear', 'organize', 'belief', 'accord', '68', 'length', 'prove', 'time', 'huge', 'attitude', 'stay', 'colorado', 'greed', 'immediate', 'domestic', 'head', 'design', 'recession', 'americas', 'receive', 'conflict', 'paulson', 'waste', 'honestly', 'nine', 'proabortion', 'white', '50000', 'race', 'focus', 'francis', 'little', 'confront', 'middleclass', 'impressive', 'anybody', 'short', 'kelly', 'fight', 'brave', 'ethic', 'supporter', 'ongoing', 'tuition', 'warren', 'balance', 'mission', '90', 'science', 'wait', 'exciting', 'terrific', 'estimate', 'raise', 'tirelessly', 'troop', 'remove', 'attorney', 'service', 'onto', 'benghazi', 'clock', 'keep', '2017', 'east', 'near', 'simply', 'unite', 'confident', 'least', 'kid', 'ladder', 'potential', 'strong', 'offense', 'announce', 'color', 'earlier', 'medicaid', '700', '30', '12', 'many', 'grade', 'equivalent', 'step', 'affordability', 'shape', 'voice', 'income', 'amendment', 'meeting', 'mandate', 'grandparent', 'four_year', 'value', 'division', 'virtually', 'perhaps', 'couple', 'dc', 'drive', 'commander', 'civil', 'escalate', 'republicans', 'negotiate', 'hotel', 'come', 'attention', 'strip', '55', 'mind', 'participate', 'jim', 'gov', 'tight', '5000', 'night', 'mortgage', 'impact', 'corp', 'financial', 'tax', 'beginning', 'onethird', 'massacre', 'research', 'age', 'sadly', 'deficit', 'never', 'voter', 'answer', 'structure', 'finally', 'unfair', 'honor', 'close', 'share', 'ability', 'information', 'saudi', 'understand', '20000', 'across', 'seem', 'ronald', 'fighting', 'others', 'earn', 'far', 'room', 'quit', 'decade', 'underscore', 'ready', 'qualify', 'presidency', 'talk', 'july', 'always', '42000', 'comfortable', 'early', 'heartache', 'politically', 'election', 'negative', 'congratulate', 'commandment', 'enable', 'capacity', 'class', 'discussion', '125000', 'unequivocally', 'congressman', 'situation', 'governor', 'set', 'ceo', 'concern', 'possibility', 'struggle', 'hofstra', 'january', 'point', 'incredibly', 'kill', 'funding', 'must', 'shadow', 'illegal', '600', 'sell', 'wellness', 'massive', 'instance', 'response', 'overboard', 'firsthand', 'presidential', 'feel', 'economic', 'access', 'treat', 'encourage', 'green', 'help', 'daughter', 'preexisting', 'root', 'ask', 'belong', 'thank', 'recklessness', 'prior', 'announcement', 'trump', 'america', 'victory', 'institution', 'safe', 'ago', 'troubled', 'korea', 'allow', 'friend', 'bigotry', 'make', 'war', 'iran', 'gap', 'school', 'warm', 'appoint', '21st', 'bill', 'life', 'plus', 'shot', 'someone', 'surgery', 'terrible', 'upon', 'standard', 'owe', 'exploit', 'year_ago', 'japan', 'offshore', 'army', 'straight', 'commission', 'hold', 'roe', 'news', 'press', 'honest', 'word', 'church', 'michigan', 'personal', 'lesson', 'decide', 'experience', 'relationship', 'dislike', 'partnership', '100000', 'activist', 'thought', 'almost', 'million', 'year', 'reform', 'member', 'reject', 'dead', 'entitle', 'pay', 'seven', 'imprison', 'produce', 'quota', 'corner', 'keystone', 'repeat', 'healthcare', 'yesterday', 'code', 'small_business', 'term', 'biden', 'side', 'heart', 'business', 'literally', 'draw', 'democrat', '35', 'privileged', 'rather', 'message', 'chicago', 'democracy', 'merit', 'constitutional', 'well', 'happy', 'nra', 'name', 'broad', 'throw', 'every', 'observer', 'man', 'indeed', 'agency', 'write', 'smarter', 'sit', 'mess', 'look', 'stage', 'associate', 'enforcement', 'student', 'therefore', 'aspect', 'medium', 'graduate', 'senator', 'rate', 'tradition', 'region', 'star', '60', 'recognize', 'despite', 'international', 'criterion', 'solution', 'please', 'allegation', 'pray', 'effective', 'pipeline', 'respect', 'alive', 'still', 'partialbirth', 'care', 'poor', 'corporation', 'okay', 'desire', 'earth', 'two', 'fundamental', 'main', 'decline', 'small', 'russia', 'instead', 'canada', 'everyday', 'competition', 'accountability', 'russian', 'aware', 'impression', 'involve', 'aid', 'relentlessly', 'colombia', 'mine', 'clearly', 'possible', 'factory', '4', 'jones', 'like', 'lay', 'disappointment', 'problem', 'identify', 'back', 'religion', 'passion', 'low', 'appreciate', 'poll', 'city', 'opposition', 'provide', 'tomorrow', 'reauthorize', 'conclusion', 'lucky', 'facility', 'rise', 'poison', 'underlie', 'terrorism', 'deliver', 'ship', 'recently', 'politics', 'president', 'consistent', 'complicated', 'washington', 'separate', 'oil', 'essentially', 'confirm', 'group', 'english', 'net', 'plane', 'person', 'cost', 'less', 'eventually', 'appropriate', 'meet', 'failed', 'lawyer', 'faith'}"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Rep_pool = set(Rep_model.wv.vocab.keys())\n",
    "Dem_pool = set(Dem_model.wv.vocab.keys())\n",
    "print(Rep_pool.intersection(Dem_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('convention', 0.9980177283287048),\n",
       " ('deal', 0.9979611039161682),\n",
       " ('last', 0.9979349970817566),\n",
       " ('send', 0.9979149103164673),\n",
       " ('democrat', 0.9979007840156555),\n",
       " ('message', 0.9978934526443481),\n",
       " ('opportunity', 0.9978873133659363),\n",
       " ('individual', 0.9978849291801453),\n",
       " ('hope', 0.9978833198547363),\n",
       " ('iran', 0.9978702068328857)]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rep_model.wv.most_similar(positive=[\"republican\",'great'],negative=['democratic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('young', 0.9993492364883423),\n",
       " ('health_care', 0.999322772026062),\n",
       " ('system', 0.9993202686309814),\n",
       " ('interest', 0.9993100166320801),\n",
       " ('away', 0.9992996454238892),\n",
       " ('kid', 0.9992901086807251),\n",
       " ('cut', 0.9992859363555908),\n",
       " ('little', 0.9992716312408447),\n",
       " ('hour', 0.9992683529853821),\n",
       " ('play', 0.9992647171020508)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dem_model.wv.most_similar(positive=['democratic','great'],negative=['republican'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d. Provide the top 30 predicted output words for a series of words/tokens based on your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kansas', 0.00068603543),\n",
       " ('trust', 0.0006403404),\n",
       " ('energy', 0.0006362638),\n",
       " ('party', 0.0006152094),\n",
       " ('existential', 0.000599299),\n",
       " ('socioeconomic', 0.00059543166),\n",
       " ('spring', 0.0005950942),\n",
       " ('fight', 0.0005873365),\n",
       " ('warrior', 0.00057709054),\n",
       " ('rare', 0.00056402874),\n",
       " ('freedom', 0.000556502),\n",
       " ('jeopardy', 0.0005560527),\n",
       " ('mom', 0.00055372505),\n",
       " ('gender', 0.0005479103),\n",
       " ('timeline', 0.00053967576),\n",
       " ('lowcost', 0.0005386202),\n",
       " ('protest', 0.0005295031),\n",
       " ('double', 0.0005220721),\n",
       " ('holiday', 0.00051562046),\n",
       " ('american', 0.0005123059),\n",
       " ('next', 0.0005060141),\n",
       " ('regulation', 0.0005037683),\n",
       " ('unbundle', 0.0005036505),\n",
       " ('onto', 0.0004889844),\n",
       " ('pool', 0.00048536542),\n",
       " ('grandkids', 0.00048443698),\n",
       " ('reaganlike', 0.000481958),\n",
       " ('1012', 0.00048182756),\n",
       " ('hardest', 0.00047990138),\n",
       " ('support_candidate', 0.00047845615)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rep_model.predict_output_word([\"make\",\"america\",\"great\"], topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gap', 0.0008641921),\n",
       " ('math', 0.00080421846),\n",
       " ('requirement', 0.0007871303),\n",
       " ('rigorous', 0.0007852309),\n",
       " ('unfunded', 0.00078021444),\n",
       " ('motor', 0.00077243405),\n",
       " ('68', 0.00077205116),\n",
       " ('42000', 0.0007463777),\n",
       " ('mandate', 0.0007407947),\n",
       " ('tired', 0.0007349613),\n",
       " ('original', 0.00073265715),\n",
       " ('ranking', 0.0007237611),\n",
       " ('perpetrate', 0.0007193926),\n",
       " ('turbine', 0.0007146641),\n",
       " ('prochoice', 0.0007146525),\n",
       " ('unaffordable', 0.0007135297),\n",
       " ('lifting', 0.0007107125),\n",
       " ('suppose', 0.00070941605),\n",
       " ('deployment', 0.0007052809),\n",
       " ('organize', 0.00070410135),\n",
       " ('20000', 0.0007040752),\n",
       " ('acre', 0.0006909125),\n",
       " ('generally', 0.00068605633),\n",
       " ('northwestern', 0.0006832074),\n",
       " ('swiftly', 0.00068274734),\n",
       " ('highquality', 0.0006789532),\n",
       " ('unintended', 0.0006775786),\n",
       " ('reprioritize', 0.0006767195),\n",
       " ('mccainkerry', 0.0006764001),\n",
       " ('perfectly', 0.0006738936)]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dem_model.predict_output_word([\"make\",\"america\",\"great\"], topn=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.In a short paragraph (roughly 3-5 sentences), provide your thoughts on word vectors in the SpaCy model, your thoughts on building the model in gensim and what the model identifies as “similar” terms, and your thoughts on word vectors in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not meaningful for providing words with uppercase letter in the same words in the SpaCy model.The model in gensim may be more helpful in analyzing text data as it is built with my data and the results are closely related with the context. I would select only nouns for future analysis which might better reflect topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
